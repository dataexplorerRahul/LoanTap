{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043a7047",
   "metadata": {},
   "source": [
    "In EDA (Step-1), we explored the dataset using statistics, visualizations & testing various hypotheses.\n",
    "\n",
    "Next step is to clean & preprocess the data before sending the data as input to ML algorithms. This involves various steps which are as follows:\n",
    "* Data Cleaning\n",
    "* Feature Engineering\n",
    "* Handling Outliers\n",
    "* Handling Missing values\n",
    "* Encoding Categorical features\n",
    "* Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf9f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db77a494",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9ed13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>application_type</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>address</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18500.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>10.65</td>\n",
       "      <td>340.24</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>CNMI Government</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>OWN</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7530 Barnes Flat Apt. 584\\r\\nWhitetown, NV 30723</td>\n",
       "      <td>Fully Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13175.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>16.55</td>\n",
       "      <td>466.78</td>\n",
       "      <td>D</td>\n",
       "      <td>D2</td>\n",
       "      <td>customer service / account rep</td>\n",
       "      <td>4 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443 Rice Views Apt. 282\\r\\nNorth Jameshaven, A...</td>\n",
       "      <td>Fully Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>17.86</td>\n",
       "      <td>886.11</td>\n",
       "      <td>D</td>\n",
       "      <td>D5</td>\n",
       "      <td>Branch Manager</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20239.0</td>\n",
       "      <td>57.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>w</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3857 Christopher Courts Suite 005\\r\\nEast Chri...</td>\n",
       "      <td>Charged Off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20400.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>12.12</td>\n",
       "      <td>678.75</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>California Dept of transportation</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12717.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>840 Parks Viaduct\\r\\nLake Brittanyside, MT 48052</td>\n",
       "      <td>Fully Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>17.57</td>\n",
       "      <td>880.61</td>\n",
       "      <td>D</td>\n",
       "      <td>D4</td>\n",
       "      <td>Air Traffic Control Specialist</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14572.0</td>\n",
       "      <td>63.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>w</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>042 Jamie Grove\\r\\nEast Maryshire, LA 70466</td>\n",
       "      <td>Charged Off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt        term  int_rate  installment grade sub_grade  \\\n",
       "0    18500.0   60 months     10.65       340.24     B        B2   \n",
       "1    13175.0   36 months     16.55       466.78     D        D2   \n",
       "2    35000.0   60 months     17.86       886.11     D        D5   \n",
       "3    20400.0   36 months     12.12       678.75     B        B3   \n",
       "4    35000.0   60 months     17.57       880.61     D        D4   \n",
       "\n",
       "                           emp_title emp_length home_ownership  annual_inc  \\\n",
       "0                    CNMI Government  10+ years            OWN     40000.0   \n",
       "1     customer service / account rep    4 years           RENT     30000.0   \n",
       "2                     Branch Manager  10+ years       MORTGAGE     80000.0   \n",
       "3  California Dept of transportation  10+ years           RENT     65000.0   \n",
       "4     Air Traffic Control Specialist  10+ years           RENT    200000.0   \n",
       "\n",
       "   ... pub_rec revol_bal revol_util total_acc  initial_list_status  \\\n",
       "0  ...     0.0       8.0        0.1      27.0                    f   \n",
       "1  ...     1.0    1046.0       15.8       8.0                    f   \n",
       "2  ...     0.0   20239.0       57.5      36.0                    w   \n",
       "3  ...     0.0   12717.0       49.4      31.0                    f   \n",
       "4  ...     0.0   14572.0       63.1       8.0                    w   \n",
       "\n",
       "  application_type  mort_acc  pub_rec_bankruptcies  \\\n",
       "0       INDIVIDUAL       NaN                   0.0   \n",
       "1       INDIVIDUAL       0.0                   0.0   \n",
       "2       INDIVIDUAL       2.0                   0.0   \n",
       "3       INDIVIDUAL       0.0                   0.0   \n",
       "4       INDIVIDUAL       0.0                   0.0   \n",
       "\n",
       "                                             address  loan_status  \n",
       "0   7530 Barnes Flat Apt. 584\\r\\nWhitetown, NV 30723   Fully Paid  \n",
       "1  443 Rice Views Apt. 282\\r\\nNorth Jameshaven, A...   Fully Paid  \n",
       "2  3857 Christopher Courts Suite 005\\r\\nEast Chri...  Charged Off  \n",
       "3   840 Parks Viaduct\\r\\nLake Brittanyside, MT 48052   Fully Paid  \n",
       "4        042 Jamie Grove\\r\\nEast Maryshire, LA 70466  Charged Off  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = \"../data/train.csv\"\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH, index_col=0)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f846eca",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "In this step, we'll: \n",
    "* Change data-types of certain features to relevant data-type\n",
    "* Extract relevant information from features\n",
    "    * Extract State-code from the `address` feature. Could also extract pincode, but upon careful inspection of pincodes in EDA step, not all pincodes were valid.\n",
    "* Reduce cardinality of categorical features by merging rare categories\n",
    "* Drop irrelevant features\n",
    "    * Dropping `title` as it contains null values and also its \n",
    "    information is already captured by purpose feature(no missing values)\n",
    "    * `emp_title` has very high cardinality. To use this feature, might require to group the titles based on occupation such as management, medical, education etc. But dropping it for now.\n",
    "* Encoding target feature\n",
    "    * We'll encode `loan_status` class-labels as follows: __Charged Off = 1 & Fully Paid = 0__ (assigning Charged Off as positive label because its the minority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a512ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Modifying the new copy of dataframe\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Converting issue_d & earliest_cr_line to datetime type\n",
    "    for feat in [\"issue_d\", \"earliest_cr_line\"]:    \n",
    "        df_copy[feat] = pd.to_datetime(df_copy[feat], format=\"mixed\")\n",
    "\n",
    "    # Extract state code from address feature and dropping address feature later\n",
    "    df_copy[\"state\"] = df_copy[\"address\"].str.extract(r'.\\s([\\w]{2})\\s\\d{4,5}$')[0]\n",
    "\n",
    "    # Merge categories to reduce cardinality\n",
    "    df_copy[\"home_ownership\"] = df_copy[\"home_ownership\"].replace([\"ANY\", \"NONE\"], \"OTHER\") # Merging ANY & NONE into OTHER\n",
    "    df_copy[\"verification_status\"] = df_copy[\"verification_status\"].replace(\"Source Verified\", \"Verified\") # Merging Source Verified into Verified\n",
    "    df_copy[\"purpose\"] = df_copy[\"purpose\"].apply(lambda x: x if x in {\"credit_card\", \"debt_consolidation\"} else \"other\") # Merging rarer categories into other\n",
    "    df_copy[\"application_type\"] = df_copy[\"application_type\"].replace([\"JOINT\", \"DIRECT_PAY\"], \"NON_INDIVIDUAL\")\n",
    "\n",
    "    # Encoding target_feature\n",
    "    df_copy[\"loan_status\"] = df_copy[\"loan_status\"].map({\"Charged Off\": 1, \"Fully Paid\": 0})\n",
    "\n",
    "    # Dropping features\n",
    "    df_copy.drop(columns=[\"title\", \"emp_title\", \"address\"], inplace=True)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5340364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>application_type</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18500.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>10.65</td>\n",
       "      <td>340.24</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>OWN</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13175.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>16.55</td>\n",
       "      <td>466.78</td>\n",
       "      <td>D</td>\n",
       "      <td>D2</td>\n",
       "      <td>4 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>17.86</td>\n",
       "      <td>886.11</td>\n",
       "      <td>D</td>\n",
       "      <td>D5</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20239.0</td>\n",
       "      <td>57.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>w</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>AR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20400.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>12.12</td>\n",
       "      <td>678.75</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12717.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>MT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>17.57</td>\n",
       "      <td>880.61</td>\n",
       "      <td>D</td>\n",
       "      <td>D4</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14572.0</td>\n",
       "      <td>63.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>w</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt        term  int_rate  installment grade sub_grade emp_length  \\\n",
       "0    18500.0   60 months     10.65       340.24     B        B2  10+ years   \n",
       "1    13175.0   36 months     16.55       466.78     D        D2    4 years   \n",
       "2    35000.0   60 months     17.86       886.11     D        D5  10+ years   \n",
       "3    20400.0   36 months     12.12       678.75     B        B3  10+ years   \n",
       "4    35000.0   60 months     17.57       880.61     D        D4  10+ years   \n",
       "\n",
       "  home_ownership  annual_inc verification_status  ... pub_rec revol_bal  \\\n",
       "0            OWN     40000.0            Verified  ...     0.0       8.0   \n",
       "1           RENT     30000.0            Verified  ...     1.0    1046.0   \n",
       "2       MORTGAGE     80000.0            Verified  ...     0.0   20239.0   \n",
       "3           RENT     65000.0            Verified  ...     0.0   12717.0   \n",
       "4           RENT    200000.0            Verified  ...     0.0   14572.0   \n",
       "\n",
       "   revol_util total_acc  initial_list_status  application_type  mort_acc  \\\n",
       "0         0.1      27.0                    f        INDIVIDUAL       NaN   \n",
       "1        15.8       8.0                    f        INDIVIDUAL       0.0   \n",
       "2        57.5      36.0                    w        INDIVIDUAL       2.0   \n",
       "3        49.4      31.0                    f        INDIVIDUAL       0.0   \n",
       "4        63.1       8.0                    w        INDIVIDUAL       0.0   \n",
       "\n",
       "   pub_rec_bankruptcies  loan_status state  \n",
       "0                   0.0            0    NV  \n",
       "1                   0.0            0    AL  \n",
       "2                   0.0            1    AR  \n",
       "3                   0.0            0    MT  \n",
       "4                   0.0            1    LA  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = clean_data(train_df)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d048579",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This step involves creating/engineering new features, using a combination of original features, that are meaningful & would be helpful in prediction. The features we'll engineer based on the analysis in Step-1 are as follows:\n",
    "* loan_to_income = loan_amnt / annual_inc\n",
    "* emi_to_income = installment / (annual_inc/12)\n",
    "* credit_age_years = (issue_d - earliest_cr_line)/365\n",
    "* closed_acc = total_acc - open_acc\n",
    "* negative_rec = (pub_rec > 0) | (pub_rec_bankruptcies > 0) \n",
    "* credit_util_ratio = revol_bal / annual_inc\n",
    "* mort_ratio = mort_acc / total_acc\n",
    "\n",
    "__Note:__ negative_rec looks like a binary feature and can be treated as a categorical feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f2726",
   "metadata": {},
   "source": [
    "# Handle Outliers\n",
    "\n",
    "To treat outliers for numerical features, there most commonly used approach is capping outliers on either side by bounds. To cap outliers different techniques are:\n",
    "* __IQR method__: Uses IQR to find upper & lower bounds and then cap outliers outside these bounds to the bound values. $$\\text{lower-bound} = Q1 - 1.5*IQR \\newline \\text{upper-bound} = Q3 + 1.5*IQR \\newline \\text{IQR} = Q3-Q1$$\n",
    "But, for few feature such as `pub_rec` & `pub_rec_bankruptcies`, the upper bound turns out to be 0. So, the all values after capping for these features will be 0, thus losing important information.\n",
    "* __Winsorization__: Uses fixed percentile values, i.e. 0-5th percentile for lower-bound & 95-99th percentile for upper bounds, to cap outliers exceeding these bounds.\n",
    "\n",
    "Since, all our numerical features are right-skewed, outliers appear only on the upper-end, which is why we'll use upper-bound (98th percentile) capping via Winsorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6109c8",
   "metadata": {},
   "source": [
    "# Handle Missing values\n",
    "\n",
    "We'll go for imputation instead of dropping missing values to avoid any loss of information. After cleaning the dataset, the features with missing values are: __revol_util, mort_acc & pub_rec_bankruptcies__ (Numerical) & __emp_length__ (Categorical).\n",
    "\n",
    "Now, there are multiple approaches to impute missing values for numerical & categorical features:\n",
    "* Numerical: Imputing missing values with central tendencies i.e. __Mean or Median__. Mean imputation isn't suitable in this case due to presence of outliers but Median is reliable value to be imputed.\n",
    "* Categorical: Imputing missing values with central tendency like __Mode__. \n",
    "\n",
    "Another alternative can be using neighbors to fill the missing values, which works well for numerical features. So, we'll be using `KNNImputer` to fill missing values for numerical features & for categorical features we'll be imputing Mode of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf0ba0",
   "metadata": {},
   "source": [
    "# Encoding Categorical features\n",
    "\n",
    "This step involves converting Categorical features into numerical format. Major Encoding techniques available are:\n",
    "* __One-hot encoding__: Represent the categories using a vector of zeros and ones, 1 indicating presence of a particular category and 0 indicating absence. Avoid this encoding, if the feature cardinality is high, as it'll lead to dimensionality curse. Also, take care of dummy variable trap i.e. each feature can be represented using a binary vector of size k-1 where k=# of categories/labels\n",
    "* __Ordinal encoding__: Represent the categories using an integer, usually ordered. This encoding is used where there is an inherent order observed among the feature categories.\n",
    "* __Frequency encoding__: Represent the categories using their frequency i.e. count of samples in the dataset. This can be good alternative where the feature has high cardinality.\n",
    "* __Target encoding__: Represent each category using the average of target feature for that particular category. This can be useful when the feature cardinality is high, but need to be careful of overfitting if not used properly.\n",
    "* __Weight of evidence encoding__: Represent each category as log of odds-ratio between positive & negative class samples in target feature. This is commonly used for credit risk analysis & binary classification problem (our use-case exactly). Also, this encoding is observed to work well with Logistic regression. Read about it [here](https://feature-engine.trainindata.com/en/1.8.x/user_guide/encoding/WoEEncoder.html)\n",
    "\n",
    "In this case, we'll use Ordinal-encoding for features which have some inherent order (i.e. __grade, sub_grade, emp_length__). For the remaining features we can perform either Target-encoding or WOE(Weight-of-evidence)-encoding, but they need to be performed dynamically for the changing train-validation splits in Cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e14d80a",
   "metadata": {},
   "source": [
    "# Feature Scaling \n",
    "\n",
    "Scaling is a good practice to bring values of all the features in the similar range, which proves beneficial specially for linear models (Logistic-regression) or distance-based models (KNN). There are multiple [scaling techniques](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling):\n",
    "\n",
    "* Standard-scaling: Shifts & scales the feature distribution to have mean=0 & standard-deviation=1. This works well if the feature is normally distributed. Its sensitive to outliers as it uses mean for scaling\n",
    "* Minmax-scaling: Scales the input feature distribution to range of [0, 1]. Its sensitive to outliers and scales inlier to a small range in such cases.\n",
    "* Robust-scaling: Its similar to Standard-scaling but it uses median & IQR instead of mean & standard-deviation, statistics which are robust to outliers, for scaling. Hence, its most commonly used for features containing outliers.\n",
    "* Box-cox transformation: Uses parametric, monotonic transformations to map any distribution as close to Gaussian distribution as possible. But this transformation is strictly for positive values only (>0)\n",
    "\n",
    "In our case, since almost all the numerical features have outliers & some features have zero-values (unsuitable for Box-cox), Robust-scaling looks like a suitable scaling-technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834e378",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "We'll be creating a pipeline for the above mentioned preprocessing steps, so that we can quickly experiment during model building with various choices. The order of preprocessing steps will be as follows:\n",
    "\n",
    "1. Missing values imputation\n",
    "2. Handling outliers\n",
    "3. Feature engineering \n",
    "4. Drop irrelevant features\n",
    "5. Encoding Categorical features\n",
    "6. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from category_encoders import TargetEncoder, WOEEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.utils.validation import check_is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a7c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature types\n",
    "target_feat = \"loan_status\"\n",
    "independent_feat = [col for col in cleaned_df.columns if col!=target_feat]\n",
    "date_feat = [\"issue_d\", \"earliest_cr_line\"]\n",
    "num_feat = [col for col in independent_feat if cleaned_df[col].dtype==\"float\"]\n",
    "cat_feat = [col for col in independent_feat if col not in set(num_feat) and col not in set(date_feat)]\n",
    "eng_feat = [\"loan_income_ratio\", \"emi_ratio\", \"credit_age_years\", \"closed_acc\", \"credit_util_ratio\", \"mortgage_ratio\"]\n",
    "\n",
    "# Features for pipelines\n",
    "final_cat_feat = cat_feat + [\"negative_rec\"] # Include binary negative_rec after feature engineering\n",
    "final_num_feat = num_feat + eng_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Feature-engineering transformer\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing to fit, return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Loan to income ratio\n",
    "        X[\"loan_income_ratio\"] = (X[\"loan_amnt\"] / X[\"annual_inc\"]).round(2)\n",
    "        # EMI to monthly income ratio\n",
    "        X[\"emi_ratio\"] = (X[\"installment\"] / (X[\"annual_inc\"]/12)).round(2)\n",
    "        # Credit-line age in years\n",
    "        X[\"credit_age_years\"] = ((X[\"issue_d\"] - X[\"earliest_cr_line\"]).dt.days / 365).round(1)\n",
    "        # Total closed accounts\n",
    "        X[\"closed_acc\"] = X[\"total_acc\"] - X[\"open_acc\"]\n",
    "        # Has negative records\n",
    "        X[\"negative_rec\"] = ((X[\"pub_rec\"]> 0) | (X[\"pub_rec_bankruptcies\"]>0)).astype(\"int\")\n",
    "        # Credit utilization ratio\n",
    "        X[\"credit_util_ratio\"] = (X[\"revol_bal\"] / X[\"annual_inc\"]).round(2)\n",
    "        # Mortgage accounts ratio\n",
    "        X[\"mortgage_ratio\"] = (X[\"mort_acc\"] / X[\"total_acc\"]).round(2)\n",
    "\n",
    "        return X\n",
    "    \n",
    "# Custom Outlier handler\n",
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a dataframe to access columns\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X should be a pandas Dataframe object\")\n",
    "        \n",
    "        # Calculating the statistics\n",
    "        self.bounds_ = {}\n",
    "        for col in self.features:\n",
    "            upper_bound = X[col].quantile(0.98)\n",
    "            self.bounds_[col] = upper_bound\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Check if fitted\n",
    "        if not self.bounds_:\n",
    "            raise RuntimeError(\"You must run fit() before transform()\")\n",
    "        \n",
    "        X = X.copy()\n",
    "        # Capping the outliers on the upper-end\n",
    "        for col in self.features:\n",
    "            upper_bound = self.bounds_[col]\n",
    "            X[col] = X[col].clip(upper=upper_bound)\n",
    "        return X\n",
    "    \n",
    "# Custom Feature dropper\n",
    "class FeatureDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # Nothing to fit, return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.drop(columns=self.features, errors=\"ignore\")\n",
    "    \n",
    "# Custom Imputer\n",
    "class Imputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, use_knn_imputation=False, num_features=None, cat_features=None):\n",
    "        self.use_knn_imputation = use_knn_imputation\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a dataframe to access columns\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X should be a pandas Dataframe object\")\n",
    "\n",
    "        # Instantiate the imputers\n",
    "        if self.use_knn_imputation:\n",
    "            self.num_imputer_ = KNNImputer()\n",
    "        else:\n",
    "            self.num_imputer_ = SimpleImputer(strategy=\"median\")\n",
    "        self.cat_imputer_ = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "        # Fit the imputers\n",
    "        if self.num_features:\n",
    "            self.num_imputer_.fit(X[self.num_features])\n",
    "        if self.cat_features:\n",
    "            self.cat_imputer_.fit(X[self.cat_features])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Check if the imputers have been fitted\n",
    "        try:\n",
    "            check_is_fitted(self.num_imputer_)\n",
    "            check_is_fitted(self.cat_imputer_)\n",
    "        except NotFittedError:\n",
    "            raise RuntimeError(\"You must run fit() before transform()\")\n",
    "\n",
    "        X = X.copy()\n",
    "        # Tranform numerical features\n",
    "        if self.num_features:\n",
    "            imputed_nums = self.num_imputer_.transform(X[self.num_features])\n",
    "            X[self.num_features] = imputed_nums\n",
    "\n",
    "        # Transform categorical features\n",
    "        if self.cat_features:\n",
    "            imputed_cats = self.cat_imputer_.transform(X[self.cat_features])\n",
    "            X[self.cat_features] = imputed_cats\n",
    "\n",
    "        return X\n",
    "    \n",
    "\n",
    "# Custom Categorical Encoder\n",
    "class CatEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoder_type=\"woe\", features=None):\n",
    "        self.encoder_type = encoder_type\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a dataframe to access columns\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X should be a pandas Dataframe object\")\n",
    "        \n",
    "        # Instantiate the encoder\n",
    "        if self.encoder_type == \"woe\":\n",
    "            self.cat_encoder_ = WOEEncoder(cols=self.features)\n",
    "        elif self.encoder_type == \"target\":\n",
    "            self.cat_encoder_ = TargetEncoder(cols=self.features)\n",
    "        else:\n",
    "            raise ValueError(\"encoder_type must be either 'woe' or 'target'\") \n",
    "        \n",
    "        # Fit the encoder\n",
    "        if self.features:\n",
    "            self.cat_encoder_.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Check if the encoder has fitted\n",
    "        try:\n",
    "            check_is_fitted(self.cat_encoder_)\n",
    "        except NotFittedError:\n",
    "            raise RuntimeError(\"You must run fit() before transform()\")\n",
    "        \n",
    "        X = X.copy()\n",
    "\n",
    "        # Encode the categorical features\n",
    "        if self.features:\n",
    "            X_trans = self.cat_encoder_.transform(X)\n",
    "\n",
    "        return X_trans\n",
    "    \n",
    "\n",
    "# Custom Scaler\n",
    "class Scaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a dataframe to access columns\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X should be a pandas Dataframe object\")\n",
    "        \n",
    "        # Instantiate the scaler\n",
    "        self.scaler_ = RobustScaler()\n",
    "\n",
    "        # Fit the scaler\n",
    "        if self.features:\n",
    "            self.scaler_.fit(X[self.features])\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Check if the scaler has fitted\n",
    "        try:\n",
    "            check_is_fitted(self.scaler_)\n",
    "        except NotFittedError:\n",
    "            raise RuntimeError(\"You must run fit() before transform()\")\n",
    "        \n",
    "        X = X.copy()\n",
    "\n",
    "        # Scale the numerical features\n",
    "        if self.features:\n",
    "            scaled_nums = self.scaler_.transform(X[self.features])\n",
    "            X[self.features] = scaled_nums\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "459d2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "def build_pipeline(model, use_imputation=True, use_outlier_capping=True, use_encoding=True, encoder_type=\"woe\", use_scaling=True, features_to_drop=[]):\n",
    "    #-----Imputation-----\n",
    "    if use_imputation:\n",
    "        imputer = Imputer(use_knn_imputation=False, \n",
    "                          num_features=num_feat,\n",
    "                          cat_features=cat_feat)\n",
    "    else:\n",
    "        imputer = \"passthrough\"\n",
    "\n",
    "    #-----Outlier handling-----\n",
    "    if use_outlier_capping:\n",
    "        outlier_capper = OutlierHandler(features=num_feat)\n",
    "    else:\n",
    "        outlier_capper = \"passthrough\"\n",
    "\n",
    "    #-----Categorical encoding-----\n",
    "    if use_encoding:\n",
    "        cat_encoder = CatEncoder(encoder_type, features=final_cat_feat)\n",
    "    else:\n",
    "        cat_encoder = \"passthrough\"\n",
    "    \n",
    "    #-----Dropping specified features-----\n",
    "    if features_to_drop:\n",
    "        feat_dropper = FeatureDropper(features=features_to_drop)\n",
    "    else:\n",
    "        feat_dropper = \"passthrough\"\n",
    "    \n",
    "    #-----Scaling-----\n",
    "    if use_scaling:\n",
    "        scaler = Scaler(features=final_num_feat)\n",
    "    else:\n",
    "        scaler = \"passthrough\"\n",
    "\n",
    "    # Preprocessing pipeline\n",
    "    final_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", imputer),\n",
    "        (\"outlier_capper\", outlier_capper),\n",
    "        (\"feature_engineer\", FeatureEngineer()),\n",
    "        (\"feature_dropper\", feat_dropper),\n",
    "        (\"cat_encoder\", cat_encoder),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    return final_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2ec1f",
   "metadata": {},
   "source": [
    "## Things I Learnt\n",
    "* sklearn's `Pipeline` is useful to automate steps that are __sequential__ in nature like in our case. We have automated all the preprocessing steps along with model fitting on the preprocessed data.\n",
    "* To create custom pipeline steps, we inherit from the following classes:\n",
    "    * __BaseEstimator__: It provides us with `fit() & transform()` which we can modify according to our requirements.\n",
    "    * __TransformerMixin__: It automatically provides the inheriting class with `fit_transform()` given that it has implementation of the `fit()` & `transform()` methods already.\n",
    "* While implementing a custom pipeline step, any attributes we initialize in the `__init__` method, should have the same name as the input argument as per sklearn's conventions. e.g. `self.features` for `features` argument. Also, attributes ending with an underscore e.g. `self.bounds_`, are reserved for fitted attributes (values calculated in fit() method)\n",
    "* Pipeline vs. ColumnTransformer\n",
    "    * Each step in `Pipeline` works on the entire input Dataframe X. We have defined a custom class for each Pipeline step, because we want certain steps to work on different subsets of the input features and not all e.g. In scaling, we want to scale only the numerical features & not the encoded categorical features.\n",
    "    * Though the above issue can be solved using `ColumnTransformer` which applies a transformation on a subset of columns (hence the name). If there are multiple transformations inside the `ColumnTransformer` pipeline, then all these transformations are executed __parallely__ and the results are concatenated. But few issues I faced with `ColumnTransformer` are: \n",
    "        * The output of each transformation is a numpy array and not a dataframe, thus losing the column information. This might lead to __column not found errors__ if you're try to access columns inside the transformer.\n",
    "        * The `remainder` argument determines what to do with the features which weren't transformed at all in the `ColumnTransformer` pipeline. __passthrough__ concatenates these columns untransformed to the transformed results, while __drop__ drops these columns from the final result. Now, if we use \"passthrough\" option for the date-related columns i.e. issue_d & earliest_cr_line, then as their data-type is different i.e. datetime[ns] they couldn't be concatenated to the transformed feature results & raised an error of data-type mismatch.\n",
    "    * Due to above reasons, I opted for using only Pipeline with custom step implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loantap (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
