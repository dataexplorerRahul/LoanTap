{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043a7047",
   "metadata": {},
   "source": [
    "In EDA (Step-1), we explored the dataset using statistics, visualizations & testing various hypotheses.\n",
    "\n",
    "Next step is to clean & preprocess the data before sending the data as input to ML algorithms. This involves various steps which are as follows:\n",
    "* Data Cleaning\n",
    "* Feature Engineering\n",
    "* Handling Outliers\n",
    "* Handling Missing values\n",
    "* Encoding Categorical features\n",
    "* Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf9f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db77a494",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9ed13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>purpose</th>\n",
       "      <th>title</th>\n",
       "      <th>dti</th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>application_type</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>address</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18500.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>10.65</td>\n",
       "      <td>340.24</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>CNMI Government</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>OWN</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Dec-2011</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt Consolidation</td>\n",
       "      <td>26.34</td>\n",
       "      <td>Jun-1996</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7530 Barnes Flat Apt. 584\\r\\nWhitetown, NV 30723</td>\n",
       "      <td>Fully Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13175.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>16.55</td>\n",
       "      <td>466.78</td>\n",
       "      <td>D</td>\n",
       "      <td>D2</td>\n",
       "      <td>customer service / account rep</td>\n",
       "      <td>4 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>May-2015</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>2.26</td>\n",
       "      <td>May-1990</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443 Rice Views Apt. 282\\r\\nNorth Jameshaven, A...</td>\n",
       "      <td>Fully Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>17.86</td>\n",
       "      <td>886.11</td>\n",
       "      <td>D</td>\n",
       "      <td>D5</td>\n",
       "      <td>Branch Manager</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Jul-2015</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>Credit card refinancing</td>\n",
       "      <td>34.38</td>\n",
       "      <td>Jul-1998</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20239.0</td>\n",
       "      <td>57.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>w</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3857 Christopher Courts Suite 005\\r\\nEast Chri...</td>\n",
       "      <td>Charged Off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20400.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>12.12</td>\n",
       "      <td>678.75</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>California Dept of transportation</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>May-2012</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>cardcars</td>\n",
       "      <td>21.30</td>\n",
       "      <td>Nov-1997</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12717.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>840 Parks Viaduct\\r\\nLake Brittanyside, MT 48052</td>\n",
       "      <td>Fully Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>17.57</td>\n",
       "      <td>880.61</td>\n",
       "      <td>D</td>\n",
       "      <td>D4</td>\n",
       "      <td>Air Traffic Control Specialist</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Apr-2015</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>5.08</td>\n",
       "      <td>Feb-2009</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14572.0</td>\n",
       "      <td>63.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>w</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>042 Jamie Grove\\r\\nEast Maryshire, LA 70466</td>\n",
       "      <td>Charged Off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  ...  loan_status\n",
       "0    18500.0  ...   Fully Paid\n",
       "1    13175.0  ...   Fully Paid\n",
       "2    35000.0  ...  Charged Off\n",
       "3    20400.0  ...   Fully Paid\n",
       "4    35000.0  ...  Charged Off\n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = \"../data/train.csv\"\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH, index_col=0)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f846eca",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "In this step, we'll: \n",
    "* Change data-types of certain features to relevant data-type\n",
    "* Strip whitespaces in values for features with __object__ datatype\n",
    "* Extract relevant information from features\n",
    "    * Extract State-code from the `address` feature. Could also extract pincode, but upon careful inspection of pincodes in EDA step, not all pincodes were valid.\n",
    "* Reduce cardinality of categorical features by merging rare categories\n",
    "* Drop irrelevant features\n",
    "    * Dropping `title` as it contains null values and also its \n",
    "    information is already captured by purpose feature(no missing values)\n",
    "    * `emp_title` has very high cardinality. To use this feature, might require to group the titles based on occupation such as management, medical, education etc. But dropping it for now.\n",
    "* Encoding target feature\n",
    "    * We'll encode `loan_status` class-labels as follows: __Charged Off = 1 & Fully Paid = 0__ (assigning Charged Off as positive label because its the minority class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d048579",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This step involves creating/engineering new features, using a combination of original features, that are meaningful & would be helpful in prediction. The features we'll engineer based on the analysis in Step-1 are as follows:\n",
    "* emi_to_income = installment / (annual_inc/12)\n",
    "* credit_age_years = (issue_d - earliest_cr_line)/365\n",
    "* closed_acc = total_acc - open_acc\n",
    "* negative_rec = (pub_rec > 0) | (pub_rec_bankruptcies > 0) \n",
    "* credit_util_ratio = revol_bal / annual_inc\n",
    "* mort_ratio = mort_acc / total_acc\n",
    "\n",
    "__Note:__ negative_rec looks like a binary feature and can be treated as a categorical feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f2726",
   "metadata": {},
   "source": [
    "# Handle Outliers\n",
    "\n",
    "To treat outliers for numerical features, there most commonly used approach is capping outliers on either side by bounds. To cap outliers different techniques are:\n",
    "* __IQR method__: Uses IQR to find upper & lower bounds and then cap outliers outside these bounds to the bound values. $$\\text{lower-bound} = Q1 - 1.5*IQR \\newline \\text{upper-bound} = Q3 + 1.5*IQR \\newline \\text{IQR} = Q3-Q1$$\n",
    "But, for few feature such as `pub_rec` & `pub_rec_bankruptcies`, the upper bound turns out to be 0. So, the all values after capping for these features will be 0, thus losing important information.\n",
    "* __Winsorization__: Uses fixed percentile values, i.e. 0-5th percentile for lower-bound & 95-99th percentile for upper bounds, to cap outliers exceeding these bounds.\n",
    "\n",
    "Since, all our numerical features are right-skewed, outliers appear only on the upper-end, which is why we'll use upper-bound (98th percentile) capping via Winsorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6109c8",
   "metadata": {},
   "source": [
    "# Handle Missing values\n",
    "\n",
    "We'll go for imputation instead of dropping missing values to avoid any loss of information. After cleaning the dataset, the features with missing values are: __revol_util, mort_acc & pub_rec_bankruptcies__ (Numerical) & __emp_length__ (Categorical).\n",
    "\n",
    "Now, there are multiple approaches to impute missing values for numerical & categorical features:\n",
    "* Numerical: Imputing missing values with central tendencies i.e. __Mean or Median__. Mean imputation isn't suitable in this case due to presence of outliers but Median is reliable value to be imputed.\n",
    "* Categorical: Imputing missing values with central tendency like __Mode__. \n",
    "\n",
    "Another alternative can be using neighbors to fill the missing values, which works well for numerical features. So, we'll be using `KNNImputer` to fill missing values for numerical features & for categorical features we'll be imputing Mode of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf0ba0",
   "metadata": {},
   "source": [
    "# Encoding Categorical features\n",
    "\n",
    "This step involves converting Categorical features into numerical format. Major Encoding techniques available are:\n",
    "* __One-hot encoding__: Represent the categories using a vector of zeros and ones, 1 indicating presence of a particular category and 0 indicating absence. Avoid this encoding, if the feature cardinality is high, as it'll lead to dimensionality curse. Also, take care of dummy variable trap i.e. each feature can be represented using a binary vector of size k-1 where k=# of categories/labels\n",
    "* __Ordinal encoding__: Represent the categories using an integer, usually ordered. This encoding is used where there is an inherent order observed among the feature categories.\n",
    "* __Frequency encoding__: Represent the categories using their frequency i.e. count of samples in the dataset. This can be good alternative where the feature has high cardinality.\n",
    "* __Target encoding__: Represent each category using the average of target feature for that particular category. This can be useful when the feature cardinality is high, but need to be careful of overfitting if not used properly.\n",
    "\n",
    "In this case, we'll use Ordinal-encoding for features which have some inherent order (i.e. __grade, sub_grade, emp_length__), while for features with low cardinality we'll use One-hot encoding and for features with high cardinality we'll can Target-encoding, but they need to be performed dynamically for the changing train-validation splits in Cross-validation. We can experiment with encoding only with Onehot for all features or Onehot + Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e14d80a",
   "metadata": {},
   "source": [
    "# Feature Scaling \n",
    "\n",
    "Scaling is a good practice to bring values of all the features in the similar range, which proves beneficial specially for linear models (Logistic-regression) or distance-based models (KNN). There are multiple [scaling techniques](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling):\n",
    "\n",
    "* Standard-scaling: Shifts & scales the feature distribution to have mean=0 & standard-deviation=1. This works well if the feature is normally distributed. Its sensitive to outliers as it uses mean for scaling\n",
    "* Minmax-scaling: Scales the input feature distribution to range of [0, 1]. Its sensitive to outliers and scales inlier to a small range in such cases.\n",
    "* Robust-scaling: Its similar to Standard-scaling but it uses median & IQR instead of mean & standard-deviation, statistics which are robust to outliers, for scaling. Hence, its most commonly used for features containing outliers.\n",
    "* Box-cox transformation: Uses parametric, monotonic transformations to map any distribution as close to Gaussian distribution as possible. But this transformation is strictly for positive values only (>0)\n",
    "\n",
    "In our case, since almost all the numerical features have outliers & some features have zero-values (unsuitable for Box-cox), Robust-scaling looks like a suitable scaling-technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f262ce",
   "metadata": {},
   "source": [
    "# Class-imbalance\n",
    "\n",
    "To mitigate the class imbalance we can use:\n",
    "* SMOTE(Synthetic minority Oversampling technique) as the name suggests is a technique used to oversample the minority class by generating synthetic samples. \n",
    "* class_weights parameter in the ML algorithm being used for training.\n",
    "\n",
    "We'll include SMOTE in the preprocessing pipeline to modify only the training data and provide an option to use it or not. We can experiment with both of the before-mentioned methods and choose according to the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834e378",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "We'll be creating a pipeline for the above mentioned preprocessing steps, so that we can quickly experiment during model building with various choices. The order of preprocessing steps will be as follows:\n",
    "\n",
    "1. Missing values imputation\n",
    "2. Handling outliers\n",
    "3. Feature engineering \n",
    "4. Drop irrelevant features\n",
    "5. Encoding Categorical features\n",
    "6. Feature Scaling\n",
    "7. Oversampling minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79ad9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from category_encoders import TargetEncoder, WOEEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Data-cleaner transformer\n",
    "class DataCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing to fit, return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a dataframe to access columns\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X should be a pandas Dataframe object\")\n",
    "        \n",
    "        X = X.copy()\n",
    "\n",
    "        # For object-dtype columns, strip any whitespaces in their values\n",
    "        for feat in X.columns:\n",
    "            if X[feat].dtype==\"object\":\n",
    "                X[feat] = X[feat].str.strip()\n",
    "\n",
    "        # Converting issue_d & earliest_cr_line to datetime type\n",
    "        for feat in [\"issue_d\", \"earliest_cr_line\"]:    \n",
    "            X[feat] = pd.to_datetime(X[feat], format=\"mixed\")\n",
    "\n",
    "        # Extract pincode from address feature\n",
    "        X[\"address\"] = X[\"address\"].str.extract(r'.(\\d{5})$')\n",
    "\n",
    "        # Merge categories to reduce cardinality\n",
    "        X[\"home_ownership\"] = X[\"home_ownership\"].replace([\"ANY\", \"NONE\"], \"OTHER\") # Merging ANY & NONE into OTHER\n",
    "        X[\"verification_status\"] = X[\"verification_status\"].replace(\"Source Verified\", \"Verified\") # Merging Source Verified into Verified\n",
    "        X[\"application_type\"] = X[\"application_type\"].replace([\"JOINT\", \"DIRECT_PAY\"], \"NON_INDIVIDUAL\")\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Custom Imputer transformer\n",
    "class Imputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, use_knn_imputation=False, num_features=None, cat_features=None):\n",
    "        self.use_knn_imputation = use_knn_imputation\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a dataframe to access columns\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X should be a pandas Dataframe object\")\n",
    "\n",
    "        # Instantiate the imputers\n",
    "        if self.use_knn_imputation:\n",
    "            self.num_imputer_ = KNNImputer()\n",
    "        else:\n",
    "            self.num_imputer_ = SimpleImputer(strategy=\"median\")\n",
    "        self.cat_imputer_ = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "        # Fit the imputers\n",
    "        if self.num_features:\n",
    "            self.num_imputer_.fit(X[self.num_features])\n",
    "        if self.cat_features:\n",
    "            self.cat_imputer_.fit(X[self.cat_features])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Check if the imputers have been fitted\n",
    "        try:\n",
    "            check_is_fitted(self.num_imputer_)\n",
    "            check_is_fitted(self.cat_imputer_)\n",
    "        except NotFittedError:\n",
    "            raise RuntimeError(\"You must run fit() before transform()\")\n",
    "\n",
    "        X = X.copy()\n",
    "        # Tranform numerical features\n",
    "        if self.num_features:\n",
    "            imputed_nums = self.num_imputer_.transform(X[self.num_features])\n",
    "            X[self.num_features] = imputed_nums\n",
    "\n",
    "        # Transform categorical features\n",
    "        if self.cat_features:\n",
    "            imputed_cats = self.cat_imputer_.transform(X[self.cat_features])\n",
    "            X[self.cat_features] = imputed_cats\n",
    "\n",
    "        return X\n",
    "    \n",
    "\n",
    "# Custom Outlier handler transformer\n",
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a dataframe to access columns\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X should be a pandas Dataframe object\")\n",
    "        \n",
    "        # Calculating the statistics\n",
    "        self.bounds_ = {}\n",
    "        for col in self.features:\n",
    "            upper_bound = X[col].quantile(0.98)\n",
    "            self.bounds_[col] = upper_bound\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Check if fitted\n",
    "        if not self.bounds_:\n",
    "            raise RuntimeError(\"You must run fit() before transform()\")\n",
    "        \n",
    "        X = X.copy()\n",
    "        # Capping the outliers on the upper-end\n",
    "        for col in self.features:\n",
    "            upper_bound = self.bounds_[col]\n",
    "            X[col] = X[col].clip(upper=upper_bound)\n",
    "        return X\n",
    "\n",
    "\n",
    "# Custom Feature-engineering transformer\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing to fit, return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a dataframe to access columns\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X should be a pandas Dataframe object\")\n",
    "        \n",
    "        X = X.copy()\n",
    "\n",
    "        # EMI to monthly income ratio\n",
    "        X[\"emi_ratio\"] = (X[\"installment\"] / (X[\"annual_inc\"]/12)).round(2)\n",
    "        # Credit-line age in years\n",
    "        X[\"credit_age_years\"] = ((X[\"issue_d\"] - X[\"earliest_cr_line\"]).dt.days / 365).round(1)\n",
    "        # Total closed accounts\n",
    "        X[\"closed_acc\"] = X[\"total_acc\"] - X[\"open_acc\"]\n",
    "        # Has negative records\n",
    "        X[\"negative_rec\"] = ((X[\"pub_rec\"]> 0) | (X[\"pub_rec_bankruptcies\"]>0)).astype(\"int\")\n",
    "        # Credit utilization ratio\n",
    "        X[\"credit_util_ratio\"] = (X[\"revol_bal\"] / X[\"annual_inc\"]).round(2)\n",
    "        # Mortgage accounts ratio\n",
    "        X[\"mortgage_ratio\"] = (X[\"mort_acc\"] / X[\"total_acc\"]).round(2)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "# Custom Categorical Encoder transformer\n",
    "class CatEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ohe_features=None, supervised_features=None):\n",
    "        self.ohe_features = ohe_features\n",
    "        self.supervised_features = supervised_features\n",
    "\n",
    "        # Ordinal categories mapping\n",
    "        self.grade_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6}\n",
    "        self.sub_grade_map = {\"A1\": 0, \"A2\": 1, \"A3\": 2, \"A4\": 3, \"A5\": 4, \"B1\": 5, \"B2\": 6, \"B3\": 7, \"B4\": 8, \"B5\": 9, \"C1\": 10, \"C2\": 11, \"C3\": 12, \"C4\": 13, \"C5\": 14, \"D1\": 15, \"D2\": 16, \"D3\": 17, \"D4\": 18, \"D5\": 19, \"E1\": 20, \"E2\": 21, \"E3\": 22, \"E4\": 23, \"E5\": 24, \"F1\": 25, \"F2\": 26, \"F3\": 27, \"F4\": 28, \"F5\": 29, \"G1\": 30, \"G2\": 31, \"G3\": 32, \"G4\": 33, \"G5\": 34}\n",
    "        self.emp_length_map = {\"< 1 year\": 0, \"1 year\": 1, \"2 years\": 2, \"3 years\": 3, \"4 years\": 4, \"5 years\": 5,  \"6 years\": 6, \"7 years\": 7, \"8 years\": 8, \"9 years\": 9, \"10+ years\": 10}\n",
    "        self.term_map = {\"36 months\": 0, \"60 months\": 1}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a dataframe to access columns\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X should be a pandas Dataframe object\")\n",
    "        \n",
    "        # Fit the encoders\n",
    "        if self.supervised_features:\n",
    "            self.sup_encoder_ = TargetEncoder(cols=self.supervised_features)\n",
    "            self.sup_encoder_.fit(X, y)\n",
    "\n",
    "        if self.ohe_features:\n",
    "            self.ohe_encoder_ = OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\")\n",
    "            self.ohe_encoder_.fit(X[self.ohe_features])\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Check if the encoder has fitted\n",
    "        if self.supervised_features and not hasattr(self, \"sup_encoder_\"):\n",
    "            raise NotFittedError(\"Target encoder is not fitted\")\n",
    "        if self.ohe_features and not hasattr(self, \"ohe_encoder_\"):\n",
    "            raise NotFittedError(\"One-hot encoder is not fitted\")\n",
    "        \n",
    "        X = X.copy()\n",
    "\n",
    "        # Supervised categorical encoding (WOE or Target)\n",
    "        if self.supervised_features:\n",
    "            X = self.sup_encoder_.transform(X)\n",
    "\n",
    "        # One-hot encoding\n",
    "        if self.ohe_features:\n",
    "            ohe_feature_names = self.ohe_encoder_.get_feature_names_out()\n",
    "            X_ohe = pd.DataFrame(data=self.ohe_encoder_.transform(X[self.ohe_features]).toarray(), \n",
    "                                 columns=ohe_feature_names,\n",
    "                                 index=X.index)\n",
    "            X = X.drop(columns=self.ohe_features, errors=\"ignore\") # drop the original ohe_features\n",
    "            X = pd.concat([X, X_ohe], axis=1) # concat transformed ohe_features to dataset\n",
    "        \n",
    "        # Ordinal encoding\n",
    "        X[\"term\"] = X[\"term\"].map(self.term_map)\n",
    "        X[\"grade\"] = X[\"grade\"].map(self.grade_map)\n",
    "        X[\"sub_grade\"] = X[\"sub_grade\"].map(self.sub_grade_map)\n",
    "        X[\"emp_length\"] = X[\"emp_length\"].map(self.emp_length_map)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "\n",
    "# Custom Scaler transformer\n",
    "class Scaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a dataframe to access columns\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"X should be a pandas Dataframe object\")\n",
    "        \n",
    "        # Instantiate the scaler\n",
    "        self.scaler_ = RobustScaler()\n",
    "\n",
    "        # Fit the scaler\n",
    "        if self.features:\n",
    "            self.scaler_.fit(X[self.features])\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Check if the scaler has fitted\n",
    "        try:\n",
    "            check_is_fitted(self.scaler_)\n",
    "        except NotFittedError:\n",
    "            raise RuntimeError(\"You must run fit() before transform()\")\n",
    "        \n",
    "        X = X.copy()\n",
    "\n",
    "        # Scale the numerical features\n",
    "        if self.features:\n",
    "            scaled_nums = self.scaler_.transform(X[self.features])\n",
    "            X[self.features] = scaled_nums\n",
    "\n",
    "        return X\n",
    "    \n",
    "\n",
    "# Custom Feature dropper transformer\n",
    "class FeatureDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # Nothing to fit, return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.drop(columns=self.features, errors=\"ignore\")\n",
    "    \n",
    "\n",
    "# Custom Dtype converter transformer\n",
    "class DtypeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing to fit, return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Convert dtype of categorical features from 'object' to 'category'\n",
    "        cat_feat = [feat for feat in X.columns if X[feat].dtype==\"object\"]\n",
    "        if cat_feat:\n",
    "            X[cat_feat] = X[cat_feat].astype(\"category\")\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "def build_pipeline(\n",
    "        numerical_features, \n",
    "        engineered_features,\n",
    "        categorical_features,\n",
    "        supervised_features,\n",
    "        onehot_features,\n",
    "        ordinal_features,\n",
    "        use_imputation=True, \n",
    "        use_outlier_capping=True, \n",
    "        use_encoding=True,\n",
    "        use_scaling=True,\n",
    "        use_smote=False, \n",
    "        features_to_drop=None,\n",
    "        convert_cat_dtype=False\n",
    "    ):\n",
    "    # Feature types\n",
    "    scaling_features = numerical_features + engineered_features + ordinal_features # including ordinal-encoded except \n",
    "\n",
    "    #-----Imputation-----\n",
    "    if use_imputation:\n",
    "        imputer = Imputer(use_knn_imputation=False, \n",
    "                          num_features=numerical_features,\n",
    "                          cat_features=categorical_features)\n",
    "    else:\n",
    "        imputer = \"passthrough\"\n",
    "\n",
    "    #-----Outlier handling-----\n",
    "    if use_outlier_capping:\n",
    "        outlier_capper = OutlierHandler(features=numerical_features)\n",
    "    else:\n",
    "        outlier_capper = \"passthrough\"\n",
    "\n",
    "    #-----Categorical encoding-----\n",
    "    if use_encoding:\n",
    "        cat_encoder = CatEncoder(ohe_features=onehot_features, supervised_features=supervised_features)\n",
    "    else:\n",
    "        cat_encoder = \"passthrough\"\n",
    "    \n",
    "    #-----Scaling-----\n",
    "    if use_scaling:\n",
    "        scaler = Scaler(features=scaling_features)\n",
    "    else:\n",
    "        scaler = \"passthrough\"\n",
    "        \n",
    "    #-----Dropping specified features-----\n",
    "    if features_to_drop:\n",
    "        feat_dropper = FeatureDropper(features=features_to_drop)\n",
    "    else:\n",
    "        feat_dropper = \"passthrough\"\n",
    "\n",
    "    #-----Oversampling-----\n",
    "    if use_smote:\n",
    "        oversampler = SMOTE()\n",
    "    else:\n",
    "        oversampler = \"passthrough\"\n",
    "\n",
    "    #-----Converting Categorical features' dtype-----\n",
    "    # Useful for algorithms like Xgboost to handle categorical features\n",
    "    if convert_cat_dtype:\n",
    "        dtype_converter = DtypeConverter()\n",
    "    else:\n",
    "        dtype_converter = \"passthrough\"\n",
    "\n",
    "    # Preprocessing pipeline\n",
    "    final_pipeline = Pipeline(steps=[\n",
    "        (\"data_cleaner\", DataCleaner()),\n",
    "        (\"imputer\", imputer),\n",
    "        (\"outlier_capper\", outlier_capper),\n",
    "        (\"feature_engineer\", FeatureEngineer()),\n",
    "        (\"cat_encoder\", cat_encoder),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"feature_dropper\", feat_dropper),\n",
    "        (\"oversampler\", oversampler),\n",
    "        (\"dtype_converter\", dtype_converter)\n",
    "    ])\n",
    "\n",
    "    return final_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2ec1f",
   "metadata": {},
   "source": [
    "## Things I Learnt\n",
    "* sklearn's `Pipeline` is useful to automate steps that are __sequential__ in nature like in our case. We have automated all the preprocessing steps along with model fitting on the preprocessed data.\n",
    "* Since, we have Oversampling as a pipeline step, we are essentially increasing/modifying the # of rows in the dataset which sklearn's `Pipeline` doesn't allow. Due to this, we have used __imbalanced-learn's__ Pipeline implementation which make it possible. To check if it actually works, comment the model step & use `.fit_resample()` method on the instantiated pipeline.\n",
    "* To create custom pipeline steps, we inherit from the following classes:\n",
    "    * __BaseEstimator__: It provides us with `fit() & transform()` which we can modify according to our requirements.\n",
    "    * __TransformerMixin__: It automatically provides the inheriting class with `fit_transform()` given that it has implementation of the `fit()` & `transform()` methods already.\n",
    "* While implementing a custom pipeline step, any attributes we initialize in the `__init__` method, should have the same name as the input argument as per sklearn's conventions. e.g. `self.features` for `features` argument. Also, attributes ending with an underscore e.g. `self.bounds_`, are reserved for fitted attributes (values calculated in fit() method)\n",
    "* Pipeline vs. ColumnTransformer\n",
    "    * Each step in `Pipeline` works on the entire input Dataframe X. We have defined a custom class for each Pipeline step, because we want certain steps to work on different subsets of the input features and not all e.g. In scaling, we want to scale only the numerical features & not the encoded categorical features.\n",
    "    * Though the above issue can be solved using `ColumnTransformer` which applies a transformation on a subset of columns (hence the name). If there are multiple transformations inside the `ColumnTransformer` pipeline, then all these transformations are executed __parallely__ and the results are concatenated. But few issues I faced with `ColumnTransformer` are: \n",
    "        * The output of each transformation is a numpy array and not a dataframe, thus losing the column information. This might lead to __column not found errors__ if you're try to access columns inside the transformer.\n",
    "        * The `remainder` argument determines what to do with the features which weren't transformed at all in the `ColumnTransformer` pipeline. __passthrough__ concatenates these columns untransformed to the transformed results, while __drop__ drops these columns from the final result. Now, if we use \"passthrough\" option for the date-related columns i.e. issue_d & earliest_cr_line, then as their data-type is different i.e. datetime[ns] they couldn't be concatenated to the transformed feature results & raised an error of data-type mismatch.\n",
    "    * Due to above reasons, I opted for using only Pipeline with custom step implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loantap (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
