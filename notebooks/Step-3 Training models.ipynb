{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e04263",
   "metadata": {},
   "source": [
    "In Step-2, we performed various data preprocessing steps necessary to make it ready for ingestion by Machine learning models.\n",
    "\n",
    "In this step, we'll:\n",
    "* Experiment training multiple models in the preprocessed data using the pipeline. The idea is to train quick-and-dirty models with standard parameters to gauge which models perform well, which features are important for these models etc. Models that we'll be experimenting with are: Logistic regression, Random Forest, XgBoost, LightGBM, SVM\n",
    "* Experiment and validate choices in preprocessing pipeline such as encoding type, scaling, outlier-capping etc.\n",
    "* Finally shortlist top-3 performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82b8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src path\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47251055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from src.utils import TRAIN_DATA_PATH\n",
    "from src.data_preprocessing.preprocessor import build_pipeline\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0e1588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>application_type</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>address</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18500.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>10.65</td>\n",
       "      <td>340.24</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>CNMI Government</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>OWN</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7530 Barnes Flat Apt. 584\\r\\nWhitetown, NV 30723</td>\n",
       "      <td>Fully Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13175.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>16.55</td>\n",
       "      <td>466.78</td>\n",
       "      <td>D</td>\n",
       "      <td>D2</td>\n",
       "      <td>customer service / account rep</td>\n",
       "      <td>4 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443 Rice Views Apt. 282\\r\\nNorth Jameshaven, A...</td>\n",
       "      <td>Fully Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>17.86</td>\n",
       "      <td>886.11</td>\n",
       "      <td>D</td>\n",
       "      <td>D5</td>\n",
       "      <td>Branch Manager</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20239.0</td>\n",
       "      <td>57.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>w</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3857 Christopher Courts Suite 005\\r\\nEast Chri...</td>\n",
       "      <td>Charged Off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20400.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>12.12</td>\n",
       "      <td>678.75</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>California Dept of transportation</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12717.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>840 Parks Viaduct\\r\\nLake Brittanyside, MT 48052</td>\n",
       "      <td>Fully Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>17.57</td>\n",
       "      <td>880.61</td>\n",
       "      <td>D</td>\n",
       "      <td>D4</td>\n",
       "      <td>Air Traffic Control Specialist</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14572.0</td>\n",
       "      <td>63.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>w</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>042 Jamie Grove\\r\\nEast Maryshire, LA 70466</td>\n",
       "      <td>Charged Off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt        term  int_rate  installment grade sub_grade  \\\n",
       "0    18500.0   60 months     10.65       340.24     B        B2   \n",
       "1    13175.0   36 months     16.55       466.78     D        D2   \n",
       "2    35000.0   60 months     17.86       886.11     D        D5   \n",
       "3    20400.0   36 months     12.12       678.75     B        B3   \n",
       "4    35000.0   60 months     17.57       880.61     D        D4   \n",
       "\n",
       "                           emp_title emp_length home_ownership  annual_inc  \\\n",
       "0                    CNMI Government  10+ years            OWN     40000.0   \n",
       "1     customer service / account rep    4 years           RENT     30000.0   \n",
       "2                     Branch Manager  10+ years       MORTGAGE     80000.0   \n",
       "3  California Dept of transportation  10+ years           RENT     65000.0   \n",
       "4     Air Traffic Control Specialist  10+ years           RENT    200000.0   \n",
       "\n",
       "   ... pub_rec revol_bal revol_util total_acc  initial_list_status  \\\n",
       "0  ...     0.0       8.0        0.1      27.0                    f   \n",
       "1  ...     1.0    1046.0       15.8       8.0                    f   \n",
       "2  ...     0.0   20239.0       57.5      36.0                    w   \n",
       "3  ...     0.0   12717.0       49.4      31.0                    f   \n",
       "4  ...     0.0   14572.0       63.1       8.0                    w   \n",
       "\n",
       "  application_type  mort_acc  pub_rec_bankruptcies  \\\n",
       "0       INDIVIDUAL       NaN                   0.0   \n",
       "1       INDIVIDUAL       0.0                   0.0   \n",
       "2       INDIVIDUAL       2.0                   0.0   \n",
       "3       INDIVIDUAL       0.0                   0.0   \n",
       "4       INDIVIDUAL       0.0                   0.0   \n",
       "\n",
       "                                             address  loan_status  \n",
       "0   7530 Barnes Flat Apt. 584\\r\\nWhitetown, NV 30723   Fully Paid  \n",
       "1  443 Rice Views Apt. 282\\r\\nNorth Jameshaven, A...   Fully Paid  \n",
       "2  3857 Christopher Courts Suite 005\\r\\nEast Chri...  Charged Off  \n",
       "3   840 Parks Viaduct\\r\\nLake Brittanyside, MT 48052   Fully Paid  \n",
       "4        042 Jamie Grove\\r\\nEast Maryshire, LA 70466  Charged Off  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH, index_col=0)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48245938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe copy to work on\n",
    "train_df_copy = train_df.copy()\n",
    "\n",
    "# Feature types\n",
    "target_feat = \"loan_status\"\n",
    "independent_feat = [col for col in train_df_copy.columns if col!=target_feat and col not in {\"title\", \"emp_title\"}] # excluding title & emp_title columns from beginning\n",
    "date_feat = [\"issue_d\", \"earliest_cr_line\"]\n",
    "num_feat = [col for col in independent_feat if train_df_copy[col].dtype==\"float\"]\n",
    "cat_feat = [col for col in independent_feat if col not in set(num_feat).union(date_feat)]\n",
    "eng_feat = [\"emi_ratio\", \"credit_age_years\", \"closed_acc\", \"credit_util_ratio\", \"mortgage_ratio\"]\n",
    "\n",
    "# Types of categorical features\n",
    "ordinal_feat = [\"grade\", \"sub_grade\", \"emp_length\"]\n",
    "supervised_feat = [\"purpose\", \"address\"] # features to receive supervised categorical encoding i.e. Target encoding\n",
    "ohe_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf685cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch X & y\n",
    "X, y = train_df_copy[independent_feat], train_df_copy[target_feat]\n",
    "\n",
    "# Encoding the target feature\n",
    "y = y.map({\"Charged Off\": 1, \"Fully Paid\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961456f7",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "We'll be using:\n",
    "* Cross-validation i.e. `StratifiedKFold` to maintain the same target label distribution across the train & validation splits. The metrics we'll be using are: F1-score & PR-AUC\n",
    "* MLFlow to track the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d449e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raon1\\Desktop\\Python\\Projects\\LoanTap\\.venv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/raon1/Desktop/Python/Projects/LoanTap/mlflow_tracking/mlruns/529194635109779319', creation_time=1765452200241, experiment_id='529194635109779319', last_update_time=1765452200241, lifecycle_stage='active', name='loantap-experiments', tags={'mlflow.experimentKind': 'custom_model_development'}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Mlflow experiment tracking\n",
    "tracking_uri_path = os.path.abspath('../mlflow_tracking/mlruns').replace('\\\\', '/')\n",
    "mlflow.set_tracking_uri(f\"file:///{tracking_uri_path}\") # Set path for experiment tracking artifacts \n",
    "mlflow.set_experiment(\"loantap-experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6e6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run cross-validation experiments & log them\n",
    "def run_cross_validation(run_name, pipeline_config, model, cv, X, y, metrics=[\"precision\", \"recall\", \"f1\", \"average_precision\"]):\n",
    "    # Fetch preprocessing pipeline\n",
    "    pipeline = build_pipeline(num_feat, eng_feat, cat_feat, supervised_feat, ohe_feat, ordinal_feat, **pipeline_config)\n",
    "    # Append model to end of pipeline steps\n",
    "    pipeline.steps.append((\"model\", model))\n",
    "    \n",
    "    # Mlflow run\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        scores = cross_validate(\n",
    "            estimator=pipeline,\n",
    "            X=X,\n",
    "            y=y,\n",
    "            cv=cv,\n",
    "            scoring=metrics,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fetch and log train & test metrics\n",
    "        metrics_dict = {\"Train\": [], \"Test\": []}\n",
    "        for metric in metrics:\n",
    "            train_metric_mean, train_metric_std = scores[f\"train_{metric}\"].mean(), scores[f\"train_{metric}\"].std()\n",
    "            test_metric_mean, test_metric_std = scores[f\"test_{metric}\"].mean(), scores[f\"test_{metric}\"].std()\n",
    "            metrics_dict[\"Train\"].append(f\"{train_metric_mean:.3f} ± {train_metric_std:.3f}\")\n",
    "            metrics_dict[\"Test\"].append(f\"{test_metric_mean:.3f} ± {test_metric_std:.3f}\")\n",
    "        \n",
    "        # Log the metrics_df\n",
    "        metrics_df = pd.DataFrame(metrics_dict, index=metrics,)\n",
    "        mlflow.log_table(metrics_df.reset_index().rename(columns={\"index\": \"Metric\"}), artifact_file=\"metrics.json\")\n",
    "        print(f\"----------{run_name}----------\")\n",
    "        print(metrics_df)\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        # Log the parameters\n",
    "        for k, v in pipeline_config.items():\n",
    "            mlflow.log_param(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d87681",
   "metadata": {},
   "source": [
    "# Experiment-1: Logistic regression\n",
    "\n",
    "We'll start with the simplest binary-classification model i.e. Logistic regression. \n",
    "\n",
    "We'll execute separate runs for the following sub-experiments \n",
    "* Keep or drop features as per VIF analysis\n",
    "* Trying out different categorical encoding type: One-hot + Target or Only One-hot\n",
    "* Keep outliers or not\n",
    "* Using SMOTE vs class_weights to address class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b751b98",
   "metadata": {},
   "source": [
    "## Dropping columns\n",
    "\n",
    "Dropping following features:\n",
    "* `grade`: This information is already captured under `sub_grade` and with more granularity\n",
    "* `total_acc`: This signal is already captured by `closed_acc` (engineered feature) & `open_acc`. Also dropping because it was used to engineer `mortgage_ratio` and thus avoid multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227bffe9",
   "metadata": {},
   "source": [
    "## VIF analysis\n",
    "But before the experiments, we need to perform multicollinearity analysis using VIF (Variance-inflation-factor). For linear models such as Logistic regression, multicollinearity messes up the feature importances. So, its important to handle it first.\n",
    "$$VIF=\\frac{1}{1-R^2}$$\n",
    "We need to only keep features with VIF<5 and drop features with VIF>=5. \n",
    "Process:\n",
    "* Calculate VIF for each feature in the dataset\n",
    "* Drop only the feature with highest VIF \n",
    "* Repeat the process until the max VIF is <5 i.e. all features have VIF<5\n",
    "\n",
    "After the analysis, it turns out that these features had large values of VIF and need to be dropped in the experiments: `loan_amnt, sub_grade, negative_rec, mort_acc, revol_bal`. The VIF values of remaining features after dropping can seen below\n",
    "\n",
    "**Note:** VIF asks us to drop `sub_grade` which can be an important signal to identify defaulters. So, instead of relying blindly on VIF analysis we'll be experimenting dropping & keeping features as per VIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77cb731f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>purpose</th>\n",
       "      <th>dti</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>...</th>\n",
       "      <th>closed_acc</th>\n",
       "      <th>negative_rec</th>\n",
       "      <th>credit_util_ratio</th>\n",
       "      <th>mortgage_ratio</th>\n",
       "      <th>verification_status_Verified</th>\n",
       "      <th>application_type_NON_INDIVIDUAL</th>\n",
       "      <th>initial_list_status_w</th>\n",
       "      <th>home_ownership_OTHER</th>\n",
       "      <th>home_ownership_OWN</th>\n",
       "      <th>home_ownership_RENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.541667</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.442244</td>\n",
       "      <td>-0.110755</td>\n",
       "      <td>-0.444444</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>-0.533333</td>\n",
       "      <td>0.207020</td>\n",
       "      <td>0.805983</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.097917</td>\n",
       "      <td>0</td>\n",
       "      <td>0.531353</td>\n",
       "      <td>0.287057</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.755556</td>\n",
       "      <td>0.207020</td>\n",
       "      <td>-1.252137</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916667</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.916667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.747525</td>\n",
       "      <td>1.605332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.167457</td>\n",
       "      <td>1.491453</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.199670</td>\n",
       "      <td>0.953441</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.207020</td>\n",
       "      <td>0.375214</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.916667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.699670</td>\n",
       "      <td>1.588041</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>3.022222</td>\n",
       "      <td>0.207020</td>\n",
       "      <td>-1.011111</td>\n",
       "      <td>-0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  term  int_rate  installment  sub_grade  emp_length  annual_inc  \\\n",
       "0   0.541667     1 -0.442244    -0.110755  -0.444444    0.428571   -0.533333   \n",
       "1   0.097917     0  0.531353     0.287057   0.666667   -0.428571   -0.755556   \n",
       "2   1.916667     1  0.747525     1.605332   1.000000    0.428571    0.355556   \n",
       "3   0.700000     0 -0.199670     0.953441  -0.333333    0.428571    0.022222   \n",
       "4   1.916667     1  0.699670     1.588041   0.888889    0.428571    3.022222   \n",
       "\n",
       "    purpose       dti  open_acc  ...  closed_acc  negative_rec  \\\n",
       "0  0.207020  0.805983 -0.333333  ...    0.500000             0   \n",
       "1  0.207020 -1.252137 -0.666667  ...   -0.916667             1   \n",
       "2  0.167457  1.491453  0.333333  ...    0.916667             0   \n",
       "3  0.207020  0.375214  1.333333  ...    0.000000             0   \n",
       "4  0.207020 -1.011111 -0.833333  ...   -0.833333             0   \n",
       "\n",
       "   credit_util_ratio  mortgage_ratio  verification_status_Verified  \\\n",
       "0              -0.90       -0.090909                           1.0   \n",
       "1              -0.75       -0.454545                           1.0   \n",
       "2               0.35        0.090909                           1.0   \n",
       "3               0.10       -0.454545                           1.0   \n",
       "4              -0.55       -0.454545                           1.0   \n",
       "\n",
       "   application_type_NON_INDIVIDUAL  initial_list_status_w  \\\n",
       "0                              0.0                    0.0   \n",
       "1                              0.0                    0.0   \n",
       "2                              0.0                    1.0   \n",
       "3                              0.0                    0.0   \n",
       "4                              0.0                    1.0   \n",
       "\n",
       "   home_ownership_OTHER  home_ownership_OWN  home_ownership_RENT  \n",
       "0                   0.0                 1.0                  0.0  \n",
       "1                   0.0                 0.0                  1.0  \n",
       "2                   0.0                 0.0                  0.0  \n",
       "3                   0.0                 0.0                  1.0  \n",
       "4                   0.0                 0.0                  1.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformed dataset\n",
    "logical_features_drop = [\"grade\", \"total_acc\"]\n",
    "features_to_drop = date_feat + logical_features_drop\n",
    "pipe = build_pipeline(num_feat, eng_feat, cat_feat, supervised_feat, ohe_feat, ordinal_feat, features_to_drop=features_to_drop)\n",
    "X_trans = pipe.fit_transform(X, y)\n",
    "X_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0884a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emi_ratio</td>\n",
       "      <td>2.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>installment</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pub_rec</td>\n",
       "      <td>2.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pub_rec_bankruptcies</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>credit_util_ratio</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>1.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>home_ownership_RENT</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>open_acc</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>revol_util</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dti</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mortgage_ratio</td>\n",
       "      <td>1.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>term</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>closed_acc</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>credit_age_years</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>verification_status_Verified</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>home_ownership_OWN</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>emp_length</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>purpose</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>address</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>initial_list_status_w</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>application_type_NON_INDIVIDUAL</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>home_ownership_OTHER</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Feature   VIF\n",
       "0                         emi_ratio  2.97\n",
       "1                       installment  2.94\n",
       "2                           pub_rec  2.81\n",
       "3              pub_rec_bankruptcies  2.80\n",
       "4                        annual_inc  2.61\n",
       "5                 credit_util_ratio  1.90\n",
       "6                          int_rate  1.64\n",
       "7               home_ownership_RENT  1.51\n",
       "8                          open_acc  1.51\n",
       "9                        revol_util  1.46\n",
       "10                              dti  1.46\n",
       "11                   mortgage_ratio  1.41\n",
       "12                             term  1.37\n",
       "13                       closed_acc  1.30\n",
       "14                 credit_age_years  1.28\n",
       "15     verification_status_Verified  1.16\n",
       "16               home_ownership_OWN  1.15\n",
       "17                       emp_length  1.11\n",
       "18                          purpose  1.04\n",
       "19                          address  1.04\n",
       "20            initial_list_status_w  1.04\n",
       "21  application_type_NON_INDIVIDUAL  1.01\n",
       "22             home_ownership_OTHER  1.00"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_vif(df: pd.DataFrame):\n",
    "    # Work on copy of dataframe\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop features\n",
    "    df.drop(columns=[\"loan_amnt\", \"sub_grade\", \"negative_rec\", \"mort_acc\", \"revol_bal\"], inplace=True)\n",
    "\n",
    "    vifs = []\n",
    "    for col in df:\n",
    "        lr = LinearRegression()\n",
    "        y_vif = df[col]\n",
    "        x_vif = df.drop(col, axis=1)\n",
    "        model = lr.fit(x_vif, y_vif)\n",
    "        r2 = model.score(x_vif, y_vif)\n",
    "\n",
    "        if r2 >= 0.999:\n",
    "            vif = np.inf\n",
    "        else:\n",
    "            vif = round(1 / (1 - r2), 2)\n",
    "        vifs.append([col, vif])\n",
    "\n",
    "    vifs = pd.DataFrame(vifs, columns=[\"Feature\", \"VIF\"]).sort_values(\"VIF\", axis=0, ascending=False).reset_index(drop=True)\n",
    "    return vifs\n",
    "    \n",
    "calculate_vif(df=X_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f7b54",
   "metadata": {},
   "source": [
    "## Keeping vs Dropping features as per VIF\n",
    "\n",
    "This is experiment is with \n",
    "* Imputing missing values\n",
    "* Capping the outliers\n",
    "* Target-encoding (for slightly higher cardinality features) + Onehot-encoding for categorical features\n",
    "* Scaling the numerical features using `RobustScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782c9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------logreg_w_vif_dropping----------\n",
      "                           Train           Test\n",
      "precision          0.932 ± 0.002  0.932 ± 0.003\n",
      "recall             0.469 ± 0.001  0.469 ± 0.003\n",
      "f1                 0.624 ± 0.001  0.624 ± 0.003\n",
      "average_precision  0.775 ± 0.001  0.775 ± 0.003\n",
      "====================================================================================================\n",
      "----------logreg_wo_vif_dropping----------\n",
      "                           Train           Test\n",
      "precision          0.918 ± 0.001  0.918 ± 0.005\n",
      "recall             0.477 ± 0.001  0.477 ± 0.004\n",
      "f1                 0.628 ± 0.001  0.628 ± 0.004\n",
      "average_precision  0.778 ± 0.001  0.778 ± 0.003\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the model\n",
    "logical_feat_drop = date_feat + [\"grade\", \"total_acc\"]\n",
    "vif_feat_drop = [\"loan_amnt\", \"sub_grade\", \"negative_rec\", \"mort_acc\", \"revol_bal\"]\n",
    "lr_model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Pipeline config\n",
    "config = {\n",
    "    \"use_imputation\": True,\n",
    "    \"use_outlier_capping\": True,\n",
    "    \"use_encoding\": True, \n",
    "    \"use_scaling\": True, \n",
    "    \"use_smote\": False\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for drop_vif_features in [True, False]:\n",
    "    if drop_vif_features:\n",
    "        run_name = f\"logreg_w_vif_dropping\"\n",
    "        config[\"features_to_drop\"] = logical_feat_drop + vif_feat_drop\n",
    "    else:\n",
    "        run_name = f\"logreg_wo_vif_dropping\"\n",
    "        config[\"features_to_drop\"] = logical_feat_drop\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=lr_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8e231",
   "metadata": {},
   "source": [
    "## Missing values experiment run\n",
    "\n",
    "From the previous run, we know that keeping features & ignoring VIF analysis yield slightly higher F1 & PR-AUC. Hence we'll ignore VIF & keep the features for future runs.\n",
    "\n",
    "For this run we'll try:\n",
    "* Dropping the missing values\n",
    "* Imputing the missing values\n",
    "\n",
    "and check if there is improvement in performance by dropping them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4997fb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------logreg_wo_vif_impute_nan----------\n",
      "                           Train           Test\n",
      "precision          0.918 ± 0.001  0.918 ± 0.005\n",
      "recall             0.477 ± 0.001  0.477 ± 0.004\n",
      "f1                 0.628 ± 0.001  0.628 ± 0.004\n",
      "average_precision  0.778 ± 0.001  0.778 ± 0.003\n",
      "====================================================================================================\n",
      "----------logreg_wo_vif_drop_nan----------\n",
      "                           Train           Test\n",
      "precision          0.909 ± 0.001  0.909 ± 0.002\n",
      "recall             0.484 ± 0.001  0.484 ± 0.004\n",
      "f1                 0.632 ± 0.001  0.632 ± 0.004\n",
      "average_precision  0.783 ± 0.001  0.782 ± 0.002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pipeline config\n",
    "config = {\n",
    "    \"use_outlier_capping\": True,\n",
    "    \"use_encoding\": True, \n",
    "    \"use_scaling\": True, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for impute in [True, False]:\n",
    "    config[\"use_imputation\"] = impute\n",
    "    if impute:\n",
    "        run_name = f\"logreg_wo_vif_impute_nan\"\n",
    "    else:\n",
    "        run_name = f\"logreg_wo_vif_drop_nan\"\n",
    "        \n",
    "        # Dropping missing values\n",
    "        nan_rows = X.isna().any(axis=1)\n",
    "        X, y = X.loc[~nan_rows], y.loc[~nan_rows]\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=lr_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0433245",
   "metadata": {},
   "source": [
    "## Encoding experiment run\n",
    "From the previous run, we know that dropping missing values in the dataset improved F1 & PR-AUC metric (~0.4%). This suggests that the missing values were of the either type: MAR (Missing at Random) or MNAR (Missing not at Random). For now, we'll drop the missing values for future runs.\n",
    "\n",
    "For this run we'll try using:\n",
    "* Onehot + Target encoding\n",
    "* Only Onehot encoding all the categorical features \n",
    "* Only target encoding all the categorical features \n",
    "\n",
    "**Note:** These are excluding the ordinal features which are encoded separately in the encoding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ce51ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------logreg_wo_vif_drop_nan_ohe_only_encoding----------\n",
      "                           Train           Test\n",
      "precision          0.908 ± 0.002  0.908 ± 0.003\n",
      "recall             0.485 ± 0.001  0.485 ± 0.005\n",
      "f1                 0.633 ± 0.001  0.632 ± 0.004\n",
      "average_precision  0.783 ± 0.001  0.783 ± 0.002\n",
      "====================================================================================================\n",
      "----------logreg_wo_vif_drop_nan_ohe_target_encoding----------\n",
      "                           Train           Test\n",
      "precision          0.908 ± 0.001  0.908 ± 0.002\n",
      "recall             0.485 ± 0.001  0.485 ± 0.005\n",
      "f1                 0.633 ± 0.000  0.632 ± 0.004\n",
      "average_precision  0.783 ± 0.001  0.782 ± 0.002\n",
      "====================================================================================================\n",
      "----------logreg_wo_vif_drop_nan_target_only_encoding----------\n",
      "                           Train           Test\n",
      "precision          0.909 ± 0.001  0.909 ± 0.002\n",
      "recall             0.484 ± 0.001  0.484 ± 0.004\n",
      "f1                 0.632 ± 0.001  0.632 ± 0.004\n",
      "average_precision  0.783 ± 0.001  0.782 ± 0.002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Using X & y directly, as they don't contain missing values due to previous run\n",
    "# Pipeline config\n",
    "config = {\n",
    "    \"use_imputation\": False,\n",
    "    \"use_outlier_capping\": True,\n",
    "    \"use_encoding\": True, \n",
    "    \"use_scaling\": True, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for encoding_option in [\"ohe_only\", \"ohe_target\", \"target_only\"]:\n",
    "    run_name = f\"logreg_wo_vif_drop_nan_{encoding_option}_encoding\"\n",
    "    if encoding_option == \"ohe_only\":\n",
    "        supervised_feat = None\n",
    "        ohe_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "    elif encoding_option == \"ohe_target\":\n",
    "        supervised_feat = [\"purpose\", \"address\"]\n",
    "        ohe_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\"]\n",
    "    else:\n",
    "        supervised_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "        ohe_feat = None\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=lr_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d318c9",
   "metadata": {},
   "source": [
    "## Keeping vs Capping Outliers run\n",
    "From the previous run, we know that using One-hot only encoding result in negligible improvement in PR-AUC (~0.1%), hence choosing One-hot only encoding for future runs.\n",
    "\n",
    "Since, Logistic regression uses sigmoid function squishing all the output between 0-1, the effect of outliers is reduced. Not saying that outliers don't affect Logistic Regression, which is why testing it using this run. \n",
    "\n",
    "Also, to remove the influence of outliers if present, scaling is done using `RobustScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------logreg_wo_vif_drop_nan_ohe_only_w_outlier_capping----------\n",
      "                           Train           Test\n",
      "precision          0.908 ± 0.002  0.908 ± 0.003\n",
      "recall             0.485 ± 0.001  0.485 ± 0.005\n",
      "f1                 0.633 ± 0.001  0.632 ± 0.004\n",
      "average_precision  0.783 ± 0.001  0.783 ± 0.002\n",
      "====================================================================================================\n",
      "----------logreg_wo_vif_drop_nan_ohe_only_wo_outlier_capping----------\n",
      "                           Train           Test\n",
      "precision          0.904 ± 0.002  0.904 ± 0.003\n",
      "recall             0.489 ± 0.001  0.488 ± 0.005\n",
      "f1                 0.634 ± 0.001  0.634 ± 0.004\n",
      "average_precision  0.784 ± 0.001  0.783 ± 0.002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pipeline config\n",
    "supervised_feat = None\n",
    "ohe_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "config = {\n",
    "    \"use_imputation\": False,\n",
    "    \"use_encoding\": True, \n",
    "    \"use_scaling\": True, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for cap_outliers in [True, False]:\n",
    "    config[\"use_outlier_capping\"] = cap_outliers\n",
    "    if cap_outliers:\n",
    "        run_name = f\"logreg_wo_vif_drop_nan_ohe_only_w_outlier_capping\"  \n",
    "    else:\n",
    "        run_name = f\"logreg_wo_vif_drop_nan_ohe_only_wo_outlier_capping\"\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=lr_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5089088",
   "metadata": {},
   "source": [
    "## SMOTE vs. class_weights run\n",
    "From the previous run, we know that ignoring Outlier-capping improved Recall, F1 & PR-AUC, though negligibly (~0.1%). Hence, keeping outliers as they are for future runs.\n",
    "\n",
    "For this run, we'll experiment methods to handle class-imbalance i.e. SMOTE vs model `class_weights` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed0c3175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------logreg_wo_vif_drop_nan_ohe_only_wo_outlier_capping_w_smote----------\n",
      "                           Train           Test\n",
      "precision          0.530 ± 0.001  0.529 ± 0.002\n",
      "recall             0.773 ± 0.001  0.772 ± 0.005\n",
      "f1                 0.628 ± 0.001  0.628 ± 0.002\n",
      "average_precision  0.779 ± 0.001  0.779 ± 0.002\n",
      "====================================================================================================\n",
      "----------logreg_wo_vif_drop_nan_ohe_only_wo_outlier_capping_w_class_weights----------\n",
      "                           Train           Test\n",
      "precision          0.522 ± 0.001  0.522 ± 0.003\n",
      "recall             0.792 ± 0.001  0.791 ± 0.004\n",
      "f1                 0.629 ± 0.001  0.629 ± 0.003\n",
      "average_precision  0.784 ± 0.001  0.783 ± 0.002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pipeline config\n",
    "supervised_feat = None\n",
    "ohe_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "config = {\n",
    "    \"use_imputation\": False,\n",
    "    \"use_outlier_capping\": False,\n",
    "    \"use_encoding\": True,\n",
    "    \"use_scaling\": True, \n",
    "    \"features_to_drop\": logical_feat_drop\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for smote_use in [True, False]:\n",
    "    config[\"use_smote\"] = smote_use\n",
    "    if smote_use:\n",
    "        run_name = f\"logreg_wo_vif_drop_nan_ohe_only_wo_outlier_capping_w_smote\"\n",
    "    else:\n",
    "        run_name = f\"logreg_wo_vif_drop_nan_ohe_only_wo_outlier_capping_w_class_weights\"\n",
    "        lr_model = LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=lr_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7f4f0",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "From the experiments with Logistic regression, we observed:\n",
    "* Keeping features that VIF suggested to drop is better than dropping for metrics. My intuition is that since, the model has `penalty=\"l2\"` as default, the regularization takes care of the multicollinearity and prevents feature-coefficient from being unstable. Also, by dropping important feature such as `sub_grade` & `loan_amnt` would lead to loss in predictive signal making it difficult for the model to learn.\n",
    "* Using only One-hot encoding is slightly better than Target encoding features with compartively higher cardinality (7-10)\n",
    "* Ignoring Outlier capping provides better metrics than capping\n",
    "* Handling class-imbalance using SMOTE or class-weights significantly improved Recall, but Precision & F1 (small hit) took a hit while PR-AUC remained the same. \n",
    "Is it worth having higher Recall at the cost of Precision? I think so, because severity/cost of a False Negative (Failing to identify a defaulter) outweighs that of False Positive (Incorrectly predicting an eligible applicant as defaulter).\n",
    "So, we'll use `class_weights` parameter of the model to handle class-imbalance as its more efficient both computationally & performance-wise compared to SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab49d5",
   "metadata": {},
   "source": [
    "# Experiment-2: Random Forest\n",
    "\n",
    "In this section we'll experiment with Bagging model i.e. Random Forest which is more complex than Logistic Regression\n",
    "\n",
    "Decision Trees based algorithms are more robust and don't require:\n",
    "* VIF analysis as they aren't affect by multicollinearity. Multicollinearity comes into picture when linear combination of all features is used for prediction, but in case of DT based algorithms, every decision/condition evaluated at each node considers only a single feature at a time. So, we won't we be dropping any features and let the algorithm figure out which features to consider for node splitting.\n",
    "* Encoding the categorical features mostly for newer implementations. But since sklearn's implementation requires us to encode the categorical features we'll try out different encoding strategies i.e. one-hot only, one-hot + target & target only\n",
    "* Outlier handling since they are robust to them. But we'll still experiment with & without handling outlier scenarios\n",
    "* Missing value imputation as of new version implementation. The model figures out how to split samples into child nodes for feature with missing values during both training & inference time. But we'll still experiment with leaving NaNs as they are vs. dropping them.\n",
    "* Scaling because while evaluating a condition at a tree node, it considers relative ordering of samples rather than their magnitudes. So, we won't experiment for scaling\n",
    "\n",
    "From the previous experiment, it was pretty clear that using `class_weights` parameters was beneficial. So, we'll be setting `class_weight=\"balanced_subsample` for `RandomForestClassifier`, which makes sure to assign class-weight to every bootstrapped subset created for each estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4bdefa",
   "metadata": {},
   "source": [
    "## Keep vs Impute Missing values run\n",
    "\n",
    "In this experiment, we'll modify certain parameters to prevent Random-Forest model from overfitting and reduce experiment time:\n",
    "* increase number of estimators (default=100) i.e. 200\n",
    "* set a lower `max_depth` (default=None) i.e. 10\n",
    "* set a higher `min_samples_leaf` (default=1) i.e. 50 \n",
    "\n",
    "We'll keep this config for RandomForest same for all the experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2b0285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting X & y to contain missing values (i.e. original data)\n",
    "X, y = train_df_copy[independent_feat], train_df_copy[target_feat]\n",
    "\n",
    "# Encoding the target feature\n",
    "y = y.map({\"Charged Off\": 1, \"Fully Paid\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------rf_impute_nan----------\n",
      "                           Train           Test\n",
      "precision          0.463 ± 0.001  0.460 ± 0.005\n",
      "recall             0.791 ± 0.002  0.787 ± 0.002\n",
      "f1                 0.584 ± 0.000  0.581 ± 0.004\n",
      "average_precision  0.757 ± 0.001  0.753 ± 0.004\n",
      "====================================================================================================\n",
      "----------rf_keep_nan----------\n",
      "                           Train           Test\n",
      "precision          0.466 ± 0.001  0.463 ± 0.003\n",
      "recall             0.789 ± 0.001  0.785 ± 0.003\n",
      "f1                 0.586 ± 0.001  0.583 ± 0.003\n",
      "average_precision  0.758 ± 0.001  0.754 ± 0.003\n",
      "====================================================================================================\n",
      "----------rf_drop_nan----------\n",
      "                           Train           Test\n",
      "precision          0.467 ± 0.003  0.463 ± 0.003\n",
      "recall             0.796 ± 0.003  0.790 ± 0.004\n",
      "f1                 0.588 ± 0.002  0.584 ± 0.003\n",
      "average_precision  0.760 ± 0.001  0.755 ± 0.003\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the model\n",
    "logical_feat_drop = date_feat\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10, \n",
    "    min_samples_leaf=50,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Pipeline config\n",
    "config = {\n",
    "    \"use_outlier_capping\": False,\n",
    "    \"use_encoding\": True,\n",
    "    \"use_scaling\": False,\n",
    "    \"use_smote\": False, \n",
    "    \"features_to_drop\": logical_feat_drop\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for option in [\"impute\", \"keep\", \"drop\"]:\n",
    "    if option == \"impute\":\n",
    "        run_name = f\"rf_impute_nan\"\n",
    "        config[\"use_imputation\"] = True\n",
    "    elif option == \"keep\":\n",
    "        # Let the model handle the missing values\n",
    "        run_name = f\"rf_keep_nan\"\n",
    "        config[\"use_imputation\"] = False\n",
    "    else:\n",
    "        run_name = f\"rf_drop_nan\"\n",
    "        config[\"use_imputation\"] = False\n",
    "\n",
    "        # Dropping missing values\n",
    "        nan_rows = X.isna().any(axis=1)\n",
    "        X, y = X.loc[~nan_rows], y.loc[~nan_rows]\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=rf_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830fd6c",
   "metadata": {},
   "source": [
    "## Encoding experiment run\n",
    "From the previous run, we know that dropping missing values provided better performance (slightly). Hence we'll drop missing values for future runs. But, Random-Forest's performance is worse than that of Logistic-regression. Let's see if it improves with following runs.\n",
    "\n",
    "For this run we'll try using:\n",
    "* Onehot + Target encoding\n",
    "* Only Onehot encoding all the categorical features \n",
    "* Only target encoding all the categorical features \n",
    "\n",
    "**Note:** These are excluding the ordinal features which are encoded separately in the encoding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------rf_drop_nan_ohe_only_encoding----------\n",
      "                           Train           Test\n",
      "precision          0.465 ± 0.003  0.462 ± 0.003\n",
      "recall             0.797 ± 0.003  0.793 ± 0.004\n",
      "f1                 0.587 ± 0.001  0.583 ± 0.002\n",
      "average_precision  0.759 ± 0.001  0.754 ± 0.003\n",
      "====================================================================================================\n",
      "----------rf_drop_nan_ohe_target_encoding----------\n",
      "                           Train           Test\n",
      "precision          0.493 ± 0.001  0.487 ± 0.001\n",
      "recall             0.832 ± 0.001  0.823 ± 0.003\n",
      "f1                 0.619 ± 0.001  0.612 ± 0.002\n",
      "average_precision  0.790 ± 0.001  0.779 ± 0.003\n",
      "====================================================================================================\n",
      "----------rf_drop_nan_target_only_encoding----------\n",
      "                           Train           Test\n",
      "precision          0.494 ± 0.002  0.488 ± 0.001\n",
      "recall             0.831 ± 0.001  0.823 ± 0.004\n",
      "f1                 0.620 ± 0.002  0.613 ± 0.001\n",
      "average_precision  0.791 ± 0.001  0.780 ± 0.002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Using X & y directly, as they don't contain missing values due to previous run\n",
    "# Pipeline config\n",
    "config = {\n",
    "    \"use_imputation\": False,\n",
    "    \"use_outlier_capping\": False,\n",
    "    \"use_encoding\": True, \n",
    "    \"use_scaling\": False, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for encoding_option in [\"ohe_only\", \"ohe_target\", \"target_only\"]:\n",
    "    run_name = f\"rf_drop_nan_{encoding_option}_encoding\"\n",
    "    if encoding_option == \"ohe_only\":\n",
    "        supervised_feat = None\n",
    "        ohe_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "    elif encoding_option == \"ohe_target\":\n",
    "        supervised_feat = [\"purpose\", \"address\"]\n",
    "        ohe_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\"]\n",
    "    else:\n",
    "        supervised_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "        ohe_feat = None\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=rf_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40dccfa",
   "metadata": {},
   "source": [
    "## Keeping vs Capping Outliers run\n",
    "From the previous run, we know that using One-hot only encoding resulted in worse results out of the 3 runs while One-hot + Target & Target only strategies provided similar results. Looking at the comparatively poor performance with One-hot encoding involved, we'll choose Target only encoding for future runs. [This](https://community.deeplearning.ai/t/isnt-it-a-bad-idea-to-use-one-hot-encode-for-decision-tree-models/165559/3) might be the reason of bad results with One-hot encoding\n",
    "\n",
    "Since, Random-Forest algorithm is mostly robust to outliers, their effect is reduced. Not saying that outliers don't affect Random-Forest (we see the effect when the model is overfitting), which is why testing it using this run. \n",
    "\n",
    "Also, to remove the influence of outliers if present, scaling is done using `RobustScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f835567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------rf_drop_nan_target_encoding_w_outlier_capping----------\n",
      "                           Train           Test\n",
      "precision          0.494 ± 0.003  0.489 ± 0.002\n",
      "recall             0.831 ± 0.002  0.822 ± 0.004\n",
      "f1                 0.620 ± 0.002  0.613 ± 0.001\n",
      "average_precision  0.791 ± 0.000  0.781 ± 0.003\n",
      "====================================================================================================\n",
      "----------rf_drop_nan_target_encoding_wo_outlier_capping----------\n",
      "                           Train           Test\n",
      "precision          0.495 ± 0.003  0.489 ± 0.002\n",
      "recall             0.831 ± 0.002  0.822 ± 0.004\n",
      "f1                 0.620 ± 0.002  0.613 ± 0.001\n",
      "average_precision  0.790 ± 0.001  0.780 ± 0.002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pipeline config\n",
    "supervised_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "ohe_feat = None\n",
    "config = {\n",
    "    \"use_imputation\": False,\n",
    "    \"use_encoding\": True, \n",
    "    \"use_scaling\": False, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for cap_outliers in [True, False]:\n",
    "    config[\"use_outlier_capping\"] = cap_outliers\n",
    "    if cap_outliers:\n",
    "        run_name = f\"rf_drop_nan_target_encoding_w_outlier_capping\"  \n",
    "    else:\n",
    "        run_name = f\"rf_drop_nan_target_encoding_wo_outlier_capping\"\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=rf_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15544786",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "From the experiments with Random Forest, we observed:\n",
    "* Dropping the missing values was better in performance than with imputation or keeping them as is.\n",
    "* Using only Target encoding for all categorical features (except ordinal) was better than other encoding strategies.\n",
    "* Outlier capping provides better metrics slightly than not capping.\n",
    "* Random Forest with chosen parameters didn't overfit but surprisingly its performance didn't even match that of Logistic-regression. It improved only the Recall by 3.1% but at a very high cost of Precision (<50%), while F1 & PR-AUC dropped by 1.6% & 0.3% respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5912d",
   "metadata": {},
   "source": [
    "# Experiment-3: XgBoost\n",
    "\n",
    "In this section we'll experiment with Gradient Boosting model i.e. XgBoost (Extreme Gradient Boosting)\n",
    "\n",
    "For XgBoost model we'll performing the same sub-experiment runs as Random-Forest i.e. Impute or not, Encoding strategies, Handle outliers or not.\n",
    "\n",
    "XgBoost model is robust enough to handle missing values, handle categorical features without encoding & handle outliers, but we want to experiment if handling these steps on our end rather than the model improves or degrades the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5b8b8",
   "metadata": {},
   "source": [
    "## Keep vs Impute Missing values run\n",
    "\n",
    "For all the upcoming experiments with XGBClassifier, we'll use the following paramters to prevent the model from overfitting the data & hence gauge its actual performance on unseen data:\n",
    "* set higher value of `n_estimators` (default=100) i.e 200\n",
    "* set smaller value of `learning_rate` (default=0.3) i.e. 0.2\n",
    "* set smaller value of `max_depth` (default=6) i.e. 4\n",
    "* set slightly smaller value (than 1 i.e. max-value) for `subsample` & `colsample_bytree` (both default=1) representing using row-sampling & column-sampling respectively to introduce randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a1a6494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting X & y to contain missing values (i.e. original data)\n",
    "X, y = train_df_copy[independent_feat], train_df_copy[target_feat]\n",
    "\n",
    "# Encoding the target feature\n",
    "y = y.map({\"Charged Off\": 1, \"Fully Paid\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f245f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------xgb_impute_nan----------\n",
      "                           Train           Test\n",
      "precision          0.513 ± 0.001  0.504 ± 0.003\n",
      "recall             0.821 ± 0.001  0.808 ± 0.002\n",
      "f1                 0.631 ± 0.001  0.621 ± 0.003\n",
      "average_precision  0.795 ± 0.001  0.784 ± 0.003\n",
      "====================================================================================================\n",
      "----------xgb_keep_nan----------\n",
      "                           Train           Test\n",
      "precision          0.514 ± 0.001  0.505 ± 0.003\n",
      "recall             0.822 ± 0.001  0.809 ± 0.003\n",
      "f1                 0.632 ± 0.001  0.622 ± 0.003\n",
      "average_precision  0.796 ± 0.001  0.785 ± 0.003\n",
      "====================================================================================================\n",
      "----------xgb_drop_nan----------\n",
      "                           Train           Test\n",
      "precision          0.516 ± 0.002  0.507 ± 0.001\n",
      "recall             0.826 ± 0.001  0.812 ± 0.003\n",
      "f1                 0.635 ± 0.001  0.624 ± 0.001\n",
      "average_precision  0.799 ± 0.001  0.786 ± 0.002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the model\n",
    "logical_feat_drop = date_feat\n",
    "neg_pos_sample_ratio = (y==0).sum() / (y==1).sum()\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators = 200,\n",
    "    max_depth = 4,\n",
    "    learning_rate = 0.1,\n",
    "    subsample = 0.8, \n",
    "    colsample_bytree = 0.8,\n",
    "    scale_pos_weight = neg_pos_sample_ratio,\n",
    "    random_state = 42,\n",
    "    n_jobs = -1,\n",
    "    enable_categorical = True\n",
    ")\n",
    "\n",
    "# Pipeline config\n",
    "config = {\n",
    "    \"use_outlier_capping\": False,\n",
    "    \"use_encoding\": False, \n",
    "    \"use_scaling\": False, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop,\n",
    "    \"convert_cat_dtype\": True\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for option in [\"impute\", \"keep\", \"drop\"]:\n",
    "    if option == \"impute\":\n",
    "        run_name = f\"xgb_impute_nan\"  \n",
    "        config[\"use_imputation\"] = True\n",
    "    elif option == \"keep\":\n",
    "        run_name = f\"xgb_keep_nan\"\n",
    "        config[\"use_imputation\"] = False\n",
    "    else:\n",
    "        run_name = f\"xgb_drop_nan\"\n",
    "        config[\"use_imputation\"] = False\n",
    "        \n",
    "        # Dropping missing values\n",
    "        nan_rows = X.isna().any(axis=1)\n",
    "        X, y = X.loc[~nan_rows], y.loc[~nan_rows]\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=xgb_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982f857",
   "metadata": {},
   "source": [
    "## Encoding experiment run\n",
    "From the previous run, we know that dropping missing values provided better performance (slightly). Hence we'll drop missing value for future runs.\n",
    "\n",
    "For this run we'll try using:\n",
    "* Only target encoding all the categorical features \n",
    "* Skip encoding any categorical feature and letting model handle it\n",
    "\n",
    "The reasoning for the choice above is that One-hot encoding don't usually perform well with Decision trees (also seen from Random-Forest runs), hence choosing encoding strategies without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf0ab67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------xgb_drop_nan_target_encoding----------\n",
      "                           Train           Test\n",
      "precision          0.514 ± 0.002  0.506 ± 0.001\n",
      "recall             0.825 ± 0.001  0.813 ± 0.003\n",
      "f1                 0.633 ± 0.001  0.624 ± 0.002\n",
      "average_precision  0.796 ± 0.001  0.787 ± 0.003\n",
      "====================================================================================================\n",
      "----------xgb_drop_nan_no_encoding----------\n",
      "                           Train           Test\n",
      "precision          0.516 ± 0.002  0.507 ± 0.001\n",
      "recall             0.826 ± 0.001  0.812 ± 0.003\n",
      "f1                 0.635 ± 0.001  0.624 ± 0.001\n",
      "average_precision  0.799 ± 0.001  0.786 ± 0.002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Using X & y directly, as they don't contain missing values due to previous run\n",
    "# Pipeline config\n",
    "config = {\n",
    "    \"use_imputation\": False,\n",
    "    \"use_outlier_capping\": False,\n",
    "    \"use_scaling\": False, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop,\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for encode in [True, False]:\n",
    "    config[\"use_encoding\"] = encode\n",
    "    if encode:\n",
    "        run_name = f\"xgb_drop_nan_target_encoding\"\n",
    "        config[\"convert_cat_dtype\"] = False\n",
    "        supervised_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "        ohe_feat = None\n",
    "    else:\n",
    "        run_name = f\"xgb_drop_nan_no_encoding\"\n",
    "        config[\"convert_cat_dtype\"] = True\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=xgb_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbde6d6",
   "metadata": {},
   "source": [
    "## Keeping vs Capping Outliers run\n",
    "From the previous run, we know that using Target-only encoding provided better slightly better performance, so we'll use that for encoding strategy.\n",
    "\n",
    "Since, Xgboost algorithm is mostly robust to outliers, their effect is reduced. Not saying that outliers don't affect Xgboost (we see the effect when the model is overfitting), which is why testing it using this run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70513da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------xgb_drop_nan_target_encoding_w_outlier_capping----------\n",
      "                           Train           Test\n",
      "precision          0.513 ± 0.002  0.506 ± 0.002\n",
      "recall             0.825 ± 0.002  0.814 ± 0.003\n",
      "f1                 0.633 ± 0.001  0.624 ± 0.002\n",
      "average_precision  0.796 ± 0.001  0.787 ± 0.003\n",
      "====================================================================================================\n",
      "----------xgb_drop_nan_target_encoding_wo_outlier_capping----------\n",
      "                           Train           Test\n",
      "precision          0.514 ± 0.002  0.506 ± 0.001\n",
      "recall             0.825 ± 0.001  0.813 ± 0.003\n",
      "f1                 0.633 ± 0.001  0.624 ± 0.002\n",
      "average_precision  0.796 ± 0.001  0.787 ± 0.003\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pipeline config\n",
    "supervised_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "ohe_feat = None\n",
    "config = {\n",
    "    \"use_imputation\": False,\n",
    "    \"use_encoding\": True,\n",
    "    \"use_scaling\": False, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop,\n",
    "    \"convert_cat_dtype\": False\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for cap_outliers in [True, False]:\n",
    "    config[\"use_outlier_capping\"] = cap_outliers\n",
    "    if cap_outliers:\n",
    "        run_name = f\"xgb_drop_nan_target_encoding_w_outlier_capping\"  \n",
    "    else:\n",
    "        run_name = f\"xgb_drop_nan_target_encoding_wo_outlier_capping\"\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=xgb_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db7730f",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "From the experiments with Xgboost, we observed:\n",
    "* Dropping missing values was better in performance than with imputation or keeping them as is.\n",
    "* Using only Target encoding for all categorical features (except ordinal) was better than other encoding strategies.\n",
    "* Outlier capping provides better metrics slightly than not capping\n",
    "* Xgboost with the chosen parameters didn't overfit and improved Recall & PR-AUC by 2.2% & 0.4% respectively. Though the increase in Recall came at the cost of Precision & F1-score i.e. 1.6% & 0.5% drop respectively. Considering the high Recall value, its still in contention with Logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a85128",
   "metadata": {},
   "source": [
    "# Experiment 4: LightGBM\n",
    "\n",
    "In this section we'll experiment with another Gradient Boosting model i.e. LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "For LightGBM model we'll performing the same sub-experiment runs as Xgboost i.e. Impute or not, Encoding strategies, Handle outliers or not.\n",
    "\n",
    "LightGBM model is robust enough to handle missing values, handle categorical features without encoding & handle outliers, but we want to experiment if handling these steps on our end rather than the model improves or degrades the performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe35be",
   "metadata": {},
   "source": [
    "## Keep vs Impute Missing values run\n",
    "\n",
    "For all the upcoming experiments with LGBMClassifier, we'll use the following paramters to prevent the model from overfitting the data & hence gauge its actual performance on unseen data:\n",
    "* set higher value of `n_estimators` (default=100) i.e 200\n",
    "* set smaller value of `max_depth` (default=-1) i.e. 5\n",
    "* set smaller value of `max_leaf_nodes` (default=31) i.e. 20\n",
    "* set higher value of `min_samples_leaf` (default=20) i.e. 40\n",
    "* set smaller value of `learning_rate` (default=0.1) i.e. 0.05\n",
    "* set slightly smaller value (than 1 i.e. max-value) for `subsample` & `colsample_bytree` (both default=1) representing using row-sampling & column-sampling respectively to introduce randomness\n",
    "\n",
    "Similar to Xgboost, if we want LightGBM to handle the categorical features, then just convert their datatype to __category__. It automatically infers such features as categorical features as per this [Kaggle post](https://www.kaggle.com/discussions/getting-started/203471)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7b9ef7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting X & y to contain missing values (i.e. original data)\n",
    "X, y = train_df_copy[independent_feat], train_df_copy[target_feat]\n",
    "\n",
    "# Encoding the target feature\n",
    "y = y.map({\"Charged Off\": 1, \"Fully Paid\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c32c25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------lgbm_impute_nan----------\n",
      "                           Train           Test\n",
      "precision          0.504 ± 0.001  0.499 ± 0.002\n",
      "recall             0.820 ± 0.000  0.811 ± 0.002\n",
      "f1                 0.625 ± 0.001  0.618 ± 0.002\n",
      "average_precision  0.791 ± 0.001  0.783 ± 0.003\n",
      "====================================================================================================\n",
      "----------lgbm_keep_nan----------\n",
      "                           Train           Test\n",
      "precision          0.506 ± 0.002  0.500 ± 0.002\n",
      "recall             0.820 ± 0.001  0.811 ± 0.002\n",
      "f1                 0.626 ± 0.001  0.619 ± 0.002\n",
      "average_precision  0.792 ± 0.001  0.784 ± 0.003\n",
      "====================================================================================================\n",
      "----------lgbm_drop_nan----------\n",
      "                           Train           Test\n",
      "precision          0.509 ± 0.001  0.503 ± 0.001\n",
      "recall             0.822 ± 0.001  0.812 ± 0.003\n",
      "f1                 0.629 ± 0.001  0.622 ± 0.001\n",
      "average_precision  0.794 ± 0.001  0.785 ± 0.002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the model\n",
    "logical_feat_drop = date_feat\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators = 200,\n",
    "    max_depth = 5,\n",
    "    max_leaf_nodes=20,\n",
    "    min_samples_leaf=30,\n",
    "    learning_rate = 0.05,\n",
    "    subsample = 0.8, \n",
    "    colsample_bytree = 0.8,\n",
    "    subsample_freq = 1, # bagging happens after 1 iteration\n",
    "    is_unbalance = True, # for imbalanced dataset\n",
    "    random_state = 42,\n",
    "    n_jobs = -1,\n",
    ")\n",
    "\n",
    "# Pipeline config\n",
    "config = {\n",
    "    \"use_outlier_capping\": False,\n",
    "    \"use_encoding\": False, \n",
    "    \"use_scaling\": False, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop,\n",
    "    \"convert_cat_dtype\": True\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for option in [\"impute\", \"keep\", \"drop\"]:\n",
    "    if option == \"impute\":\n",
    "        run_name = f\"lgbm_impute_nan\"  \n",
    "        config[\"use_imputation\"] = True\n",
    "    elif option == \"keep\":\n",
    "        run_name = f\"lgbm_keep_nan\"  \n",
    "        config[\"use_imputation\"] = False\n",
    "    else:\n",
    "        run_name = f\"lgbm_drop_nan\"\n",
    "        config[\"use_imputation\"] = False\n",
    "\n",
    "        # Dropping missing values\n",
    "        nan_rows = X.isna().any(axis=1)\n",
    "        X, y = X.loc[~nan_rows], y.loc[~nan_rows]\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=lgbm_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25d77c",
   "metadata": {},
   "source": [
    "## Encoding experiment run\n",
    "From the previous run, we know that dropping missing values provided better performance (slightly). Hence we'll drop missing values for future runs.\n",
    "\n",
    "For this run we'll try using:\n",
    "* Only target encoding all the categorical features \n",
    "* Skip encoding any categorical feature and letting model handle it\n",
    "\n",
    "The reasoning for the choice above runs is that One-hot encoding don't usually perform well with Decision trees (also seen from Random-Forest runs), hence choosing encoding strategies without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e96a5702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------lgbm_drop_nan_target_encoding----------\n",
      "                           Train           Test\n",
      "precision          0.507 ± 0.001  0.503 ± 0.001\n",
      "recall             0.821 ± 0.001  0.815 ± 0.003\n",
      "f1                 0.627 ± 0.001  0.622 ± 0.002\n",
      "average_precision  0.792 ± 0.001  0.786 ± 0.003\n",
      "====================================================================================================\n",
      "----------lgbm_drop_nan_no_encoding----------\n",
      "                           Train           Test\n",
      "precision          0.509 ± 0.001  0.503 ± 0.001\n",
      "recall             0.822 ± 0.001  0.812 ± 0.003\n",
      "f1                 0.629 ± 0.001  0.622 ± 0.001\n",
      "average_precision  0.794 ± 0.001  0.785 ± 0.002\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Using X & y directly, as they don't contain missing values due to previous run\n",
    "# Pipeline config\n",
    "config = {\n",
    "    \"use_imputation\": False,\n",
    "    \"use_outlier_capping\": False,\n",
    "    \"use_scaling\": False, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop,\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for encode in [True, False]:\n",
    "    config[\"use_encoding\"] = encode\n",
    "    if encode:\n",
    "        run_name = f\"lgbm_drop_nan_target_encoding\"\n",
    "        config[\"convert_cat_dtype\"] = False\n",
    "        supervised_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "        ohe_feat = None\n",
    "    else:\n",
    "        run_name = f\"lgbm_drop_nan_no_encoding\"\n",
    "        config[\"convert_cat_dtype\"] = True\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=lgbm_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d4d89",
   "metadata": {},
   "source": [
    "## Keeping vs Capping Outliers run\n",
    "From the previous run, we know that using target-encoding provided better slightly better performance, so we'll use that for encoding strategy.\n",
    "\n",
    "Since, LightGBM algorithm is mostly robust to outliers, their effect is reduced. Not saying that outliers don't affect LightGBM (we see the effect when the model is overfitting), which is why testing it using this run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "721b91fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------lgbm_drop_nan_target_encoding_w_outlier_capping----------\n",
      "                           Train           Test\n",
      "precision          0.507 ± 0.001  0.503 ± 0.001\n",
      "recall             0.821 ± 0.001  0.815 ± 0.003\n",
      "f1                 0.627 ± 0.001  0.622 ± 0.001\n",
      "average_precision  0.792 ± 0.000  0.786 ± 0.003\n",
      "====================================================================================================\n",
      "----------lgbm_drop_nan_target_encoding_wo_outlier_capping----------\n",
      "                           Train           Test\n",
      "precision          0.507 ± 0.001  0.503 ± 0.001\n",
      "recall             0.821 ± 0.001  0.815 ± 0.003\n",
      "f1                 0.627 ± 0.001  0.622 ± 0.002\n",
      "average_precision  0.792 ± 0.001  0.786 ± 0.003\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pipeline config\n",
    "supervised_feat = [\"verification_status\", \"application_type\", \"initial_list_status\", \"home_ownership\", \"purpose\", \"address\"]\n",
    "ohe_feat = None\n",
    "config = {\n",
    "    \"use_imputation\": False,\n",
    "    \"use_encoding\": True,\n",
    "    \"use_scaling\": False, \n",
    "    \"use_smote\": False,\n",
    "    \"features_to_drop\": logical_feat_drop,\n",
    "    \"convert_cat_dtype\": False\n",
    "}\n",
    "\n",
    "# Experiment run\n",
    "for cap_outliers in [True, False]:\n",
    "    config[\"use_outlier_capping\"] = cap_outliers\n",
    "    if cap_outliers:\n",
    "        run_name = f\"lgbm_drop_nan_target_encoding_w_outlier_capping\"  \n",
    "    else:\n",
    "        run_name = f\"lgbm_drop_nan_target_encoding_wo_outlier_capping\"\n",
    "    run_cross_validation(\n",
    "        run_name=run_name,\n",
    "        pipeline_config=config,\n",
    "        model=lgbm_model,\n",
    "        cv=cv,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9c6ca",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "From the experiments with LightGBM, we observed:\n",
    "* Dropping missing values was better in performance than with imputation or keeping them as is.\n",
    "* Using target encoding for all categorical features (except ordinal) was better than other encoding strategies.\n",
    "* Outlier capping doesn't provide any improvement over not capping.\n",
    "* LightGBM with the chosen parameters didn't overfit and improved Recall & PR-AUC by 2.4% & 0.3% respectively. Though the increase in Recall came at the cost of Precision & F1-score i.e. 1.9% & 0.7% drop respectively. Its performance is very similar to Xgboost, but Xgboost provides more balanced F1-score (though not by much)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c7e61",
   "metadata": {},
   "source": [
    "# Final verdict\n",
    "\n",
    "Below are the best runs (validation/test set performances) for the models trained until now i.e. `Logistic regression, Random Forest, Xgboost & LightGBM`:\n",
    "\n",
    "| Model | Precision | Recall | F1-score | PR-AUC (Average Precision) |\n",
    "|:---------:|:--------:|:---------:|:---------:|:---------:|\n",
    "|  Logistic regression   |  0.522 ± 0.003   |  0.791 ± 0.004   |  0.629 ± 0.003   |  0.783 ± 0.002   |\n",
    "|  Random forest   |  0.489 ± 0.002   |  0.822 ± 0.004   |  0.613 ± 0.001   |  0.781 ± 0.003   |\n",
    "|  Xgboost   |  0.506 ± 0.002   |  0.814 ± 0.003   |  0.624 ± 0.002   |  0.787 ± 0.002   |\n",
    "|  LightGBM   |  0.503 ± 0.001   |  0.815 ± 0.003   |  0.622 ± 0.002   |  0.786 ± 0.003   |\n",
    "\n",
    "__Occam's razor: The simplest model with comparable performance is the best__\n",
    "\n",
    "Considering Occam's razor, ___Logistic regression___ is one of the best models including ___Xgboost___ if we had to select top-2. So, next we'll be performing hyperparameter tuning only for these models, optimize them and then perform ensemble prediction using them. \n",
    "\n",
    "The reason for Logistic regression performing at par with Tree-based models like Xgboost suggests that the financial data might have linear/monotonic relationship between the features, which Logistic regression is capturing well. Xgboost has better Recall but poor Precision brings down the F1-score, thus being unable to improve the metrics drastically.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loantap (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
